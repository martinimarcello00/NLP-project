{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7914bf7",
   "metadata": {},
   "source": [
    "# NLP Project\n",
    "\n",
    "*Team members*:\n",
    "- Asja Attanasio\n",
    "- Daniele Lagan√†\n",
    "- Marcello Martini\n",
    "- Gianluigi Palmisano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b74fcf",
   "metadata": {},
   "source": [
    "# ‚¨áÔ∏è Import Dataset\n",
    "\n",
    "Each record has three features:\n",
    "- context\n",
    "- question\n",
    "- answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d99b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install itables plotly datamapplot bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914afcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check if running in Kaggle\n",
    "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "if IN_KAGGLE:\n",
    "  os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "  from plotly.offline import init_notebook_mode\n",
    "  init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import itables\n",
    "\n",
    "# For interactive tables\n",
    "itables.init_notebook_mode(all_interactive=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769dedbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset using the Hugging Face datasets library\n",
    "dataset = load_dataset(\"neural-bridge/rag-dataset-12000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataset['train'].to_pandas()\n",
    "test_df = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb63d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23eb9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train set size:\", len(train_df))\n",
    "print(\"Test set size:\", len(test_df))\n",
    "print(\"Columns:\", train_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df309894",
   "metadata": {},
   "source": [
    "# üîé Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0535da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows with at least one missing value in 'context', 'question', or 'answer'\n",
    "missing_rows_train = train_df[['context', 'question', 'answer']].isnull().any(axis=1).sum()\n",
    "missing_rows_test = test_df[['context', 'question', 'answer']].isnull().any(axis=1).sum()\n",
    "\n",
    "print(\"Rows with at least one missing value in train set:\", missing_rows_train)\n",
    "print(\"Rows with at least one missing value in test set:\", missing_rows_test)\n",
    "\n",
    "# Remove null values\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e862d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "# Initialize stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize lists and variables for analysis\n",
    "context_lengths = []\n",
    "question_lengths = []\n",
    "vocab = set()\n",
    "all_tokens = []\n",
    "train_df_tokenized = []\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "    \"\"\"Tokenize, lowercase, remove punctuation and stopwords from text.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum()]  # Keep only alphanumeric tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return tokens\n",
    "\n",
    "# Function to process a single entry\n",
    "def process_entry(entry):\n",
    "    \"\"\"Process a single dataset entry to extract tokens and update statistics.\"\"\"\n",
    "    context_tokens = preprocess(entry['context'])\n",
    "    question_tokens = preprocess(entry['question'])\n",
    "    \n",
    "    # Update lengths\n",
    "    context_lengths.append(len(context_tokens))\n",
    "    question_lengths.append(len(question_tokens))\n",
    "    answer_tokens = preprocess(entry['answer'])\n",
    "    \n",
    "    # Update vocabulary and token list\n",
    "    vocab.update(context_tokens)\n",
    "    vocab.update(question_tokens)\n",
    "    all_tokens.extend(context_tokens)\n",
    "    all_tokens.extend(question_tokens)\n",
    "    train_df_tokenized.append({'context': context_tokens, 'question': question_tokens, 'answer': answer_tokens})\n",
    "\n",
    "tqdm.pandas(desc=\"Processing entries\")\n",
    "train_df.progress_apply(process_entry, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(all_tokens)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2db549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=500).generate_from_frequencies(fdist)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud of Vocabulary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71689608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the frequency distribution of the most common words\n",
    "fdist.plot(30,  title=\"Top 30 Most Frequent Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics\n",
    "print(f\"Number of documents: {len(train_df)}\")\n",
    "print(f\"Average context length: {sum(context_lengths)/len(context_lengths):.2f} tokens\")\n",
    "print(f\"Average question length: {sum(question_lengths)/len(question_lengths):.2f} tokens\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Plot distribution of context lengths\n",
    "sns.histplot(context_lengths, bins=50, kde=True)\n",
    "plt.title('Distribution of Context Lengths')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e5f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(question_lengths, bins=50, kde=True)\n",
    "plt.title(\"Question Length Distribution\")\n",
    "plt.xlabel(\"Number of Tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b95914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_words = [\"what\", \"where\", \"when\", \"why\", \"how\", \"who\", \"which\"]\n",
    "question_word_counts = {word: 0 for word in question_words}\n",
    "for question in train_df['question']:\n",
    "    tokens = word_tokenize(question.lower())\n",
    "    for word in question_words:\n",
    "        question_word_counts[word] += tokens.count(word)\n",
    "question_word_counts\n",
    "question_word_counts_df = pd.DataFrame(list(question_word_counts.items()), columns=['Question Word', 'Count'])\n",
    "question_word_counts_df = question_word_counts_df.sort_values(by='Count', ascending=False)\n",
    "# Plotting the counts of question words\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Question Word', y='Count', data=question_word_counts_df)\n",
    "plt.title('Counts of Question Words')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa9cb2",
   "metadata": {},
   "source": [
    "# üßπ Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15417845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first entry in train dataset as table without truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400877f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text, preserve_question_words=False):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text by removing punctuation, stopwords, and optionally preserving question words.\n",
    "    \"\"\"\n",
    "    # Define question words to preserve\n",
    "    question_words = {\"what\", \"where\", \"when\", \"why\", \"how\", \"who\", \"which\"}\n",
    "    \n",
    "    # Tokenize and lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Lemmatize, remove punctuation, and stopwords\n",
    "    if preserve_question_words:\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and (word not in stop_words or word in question_words)]\n",
    "    else:\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e9e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tqdm for pandas to show progress\n",
    "tqdm.pandas(desc=\"Cleaning Text\")\n",
    "# Clean the training set\n",
    "train_df['context'] = train_df['context'].progress_apply(lambda x: clean_text(x))\n",
    "train_df['question'] = train_df['question'].progress_apply(lambda x: clean_text(x, preserve_question_words=True))\n",
    "train_df['answer'] = train_df['answer'].progress_apply(lambda x: clean_text(x))\n",
    "# Clean the test set\n",
    "test_df['context'] = test_df['context'].progress_apply(lambda x: clean_text(x))\n",
    "test_df['question'] = test_df['question'].progress_apply(lambda x: clean_text(x, preserve_question_words=True))\n",
    "test_df['answer'] = test_df['answer'].progress_apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a3038",
   "metadata": {},
   "source": [
    "# üí¨ Word Embedding (Word2Vec - questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df35810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize the questions into words\n",
    "train_df['tokenized_question'] = train_df['question'].apply(lambda x: word_tokenize(x.lower()))\n",
    "\n",
    "vector_size = 5 # Size of the word vectors\n",
    "\n",
    "# Train the Word2Vec model with optimized parameters for better embedding quality\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=train_df['tokenized_question'], \n",
    "    vector_size=vector_size, \n",
    "    window=10,  # Increase the context window size\n",
    "    min_count=2,  # Ignore words that appear less frequently\n",
    "    workers=4, \n",
    "    sg=1,  # Use skip-gram model for better performance on smaller datasets\n",
    "    hs=0,  # Use negative sampling\n",
    "    negative=10,  # Number of negative samples\n",
    "    epochs=20  # Train for more epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebcf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the average word embedding for a question\n",
    "def get_average_embedding(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)  # Return a zero vector if no words are found\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Generate sentence embeddings for the questions\n",
    "train_df['question_embedding'] = train_df['tokenized_question'].apply(lambda x: get_average_embedding(x, word2vec_model))\n",
    "\n",
    "# Stack embeddings into a matrix\n",
    "question_embeddings = np.stack(train_df['question_embedding'].values)\n",
    "question_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad52582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Function to find the optimal number of clusters using the Elbow Method and Silhouette Analysis\n",
    "def find_optimal_clusters(embeddings, max_clusters=10):\n",
    "    wcss = []  # Within-Cluster Sum of Squares\n",
    "    bss = []  # Between-Cluster Sum of Squares\n",
    "    silhouette_scores = []  # Silhouette Scores\n",
    "    cluster_range = range(2, max_clusters + 1)\n",
    "    \n",
    "    # Calculate Total Sum of Squares (TSS)\n",
    "    tss = np.sum((embeddings - np.mean(embeddings, axis=0))**2)\n",
    "    \n",
    "    for n_clusters in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        kmeans.fit(embeddings)\n",
    "        \n",
    "        # Calculate WCSS\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        \n",
    "        # Calculate BSS\n",
    "        bss.append(tss - kmeans.inertia_)\n",
    "        \n",
    "        # Calculate Silhouette Score\n",
    "        silhouette_avg = silhouette_score(embeddings, kmeans.labels_)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "    \n",
    "    # Plot Elbow Method for WCSS and BSS\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cluster_range, wcss, marker='o', label='WCSS (Within-Cluster Sum of Squares)')\n",
    "    plt.plot(cluster_range, bss, marker='o', label='BSS (Between-Cluster Sum of Squares)')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Sum of Squares')\n",
    "    plt.title('Elbow Method for Optimal Clusters')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot Silhouette Scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cluster_range, silhouette_scores, marker='o', label='Silhouette Score')\n",
    "    # Plot the cluster number above each point\n",
    "    for i, score in enumerate(silhouette_scores):\n",
    "        plt.text(cluster_range[i], score, str(cluster_range[i]), fontsize=9, ha='center')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Analysis for Optimal Clusters')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return wcss, bss, silhouette_scores\n",
    "\n",
    "# Find the optimal number of clusters\n",
    "wcss, bss, silhouette_scores = find_optimal_clusters(question_embeddings, max_clusters=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "n_clusters = 4\n",
    "\n",
    "# Perform K-Means clustering with 7 clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "train_df['cluster'] = kmeans.fit_predict(question_embeddings)\n",
    "\n",
    "# Reduce dimensions of question embeddings using t-SNE for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "question_embeddings_2d = tsne.fit_transform(question_embeddings)\n",
    "\n",
    "# Plot the t-SNE results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(question_embeddings_2d[:, 0], question_embeddings_2d[:, 1], c=train_df['cluster'], cmap='viridis', s=10)\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.title(f't-SNE Visualization of Question Clusters ({n_clusters} Clusters)')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.show()\n",
    "\n",
    "# Function to extract topics for each cluster\n",
    "def get_cluster_topics(df, cluster_col, text_col, top_n=10):\n",
    "    cluster_topics = {}\n",
    "    for cluster_id, group in df.groupby(cluster_col):\n",
    "        # Tokenize all questions in the cluster\n",
    "        all_words = [word for question in group[text_col] for word in word_tokenize(question.lower())]\n",
    "        # Count word frequencies\n",
    "        word_freq = Counter(all_words)\n",
    "        # Get the top N most common words\n",
    "        cluster_topics[cluster_id] = word_freq.most_common(top_n)\n",
    "    return cluster_topics\n",
    "\n",
    "# Get topics for each cluster\n",
    "topics = get_cluster_topics(train_df, cluster_col='cluster', text_col='question', top_n=10)\n",
    "\n",
    "# Display topics for each cluster\n",
    "for cluster_id, words in topics.items():\n",
    "    print(f\"Cluster {cluster_id}:\")\n",
    "    print(\", \".join([f\"{word} ({count})\" for word, count in words]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d94cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "# Reduce dimensions of question embeddings to 3D using PCA\n",
    "pca = PCA(n_components=3)\n",
    "question_embeddings_3d = pca.fit_transform(question_embeddings)\n",
    "\n",
    "\n",
    "# Prepare a DataFrame for visualization\n",
    "visualization_df = pd.DataFrame({\n",
    "    'PCA Dimension 1': question_embeddings_3d[:, 0],\n",
    "    'PCA Dimension 2': question_embeddings_3d[:, 1],\n",
    "    'PCA Dimension 3': question_embeddings_3d[:, 2],\n",
    "    'Cluster': train_df['cluster'],\n",
    "    'Question': train_df['question']\n",
    "})\n",
    "# Create an interactive 3D scatter plot with a larger figure size\n",
    "fig = px.scatter_3d(\n",
    "    visualization_df,\n",
    "    x='PCA Dimension 1',\n",
    "    y='PCA Dimension 2',\n",
    "    z='PCA Dimension 3',\n",
    "    color='Cluster',\n",
    "    hover_data=['Question'],  # Show the question on hover\n",
    "    title='Interactive 3D Visualization of Question Clusters',\n",
    "    width=1200,  # Set the width of the plot\n",
    "    height=1000   # Set the height of the plot\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac16123",
   "metadata": {},
   "source": [
    "# üî† TF-IDF analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee18b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of tokenized dictionaries into a DataFrame \n",
    "train_df_tokenized = pd.DataFrame(train_df_tokenized, columns=['context', 'question', 'answer'])\n",
    "\n",
    "train_df_tokenized.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4925a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert tokenized lists back into strings for TF-IDF processing because TfidfVectorizer expects text input as strings\n",
    "train_df_tokenized['context_str'] = train_df_tokenized['context'].apply(lambda tokens: ' '.join(tokens))\n",
    "train_df_tokenized['question_str'] = train_df_tokenized['question'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Option 1: Apply TF-IDF on the 'context' field to compute the term frequency-inverse document frequency for words in the 'context' field\n",
    "vectorizer_context = TfidfVectorizer()\n",
    "tfidf_context = vectorizer_context.fit_transform(train_df_tokenized['context_str'])\n",
    "\n",
    "# Option 2: Apply TF-IDF on the 'question' field to compute the term frequency-inverse document frequency for words in the 'question' field\n",
    "vectorizer_question = TfidfVectorizer()\n",
    "tfidf_question = vectorizer_question.fit_transform(train_df_tokenized['question_str'])\n",
    "\n",
    "# Option 3: Apply TF-IDF on the combined 'context' and 'question' fields\n",
    "train_df_tokenized['combined'] = train_df_tokenized['context_str'] + ' ' + train_df_tokenized['question_str']\n",
    "\n",
    "# This vectorizer will compute the term frequency-inverse document frequency for words in the combined field\n",
    "vectorizer_combined = TfidfVectorizer()\n",
    "tfidf_combined = vectorizer_combined.fit_transform(train_df_tokenized['combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9641030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sparse matrix of TF-IDF values for 'context' into a pandas DataFrame\n",
    "\n",
    "tfidf_context_df = pd.DataFrame(tfidf_context.toarray(), columns=vectorizer_context.get_feature_names_out())\n",
    "tfidf_question_df = pd.DataFrame(tfidf_question.toarray(), columns=vectorizer_question.get_feature_names_out())\n",
    "tfidf_combined_df = pd.DataFrame(tfidf_combined.toarray(), columns=vectorizer_combined.get_feature_names_out())\n",
    "\n",
    "print(\"Shape of TF-IDF Context DataFrame:\", tfidf_context_df.shape)\n",
    "print(\"Shape of TF-IDF Question DataFrame:\", tfidf_question_df.shape)\n",
    "print(\"Shape of TF-IDF Combined DataFrame:\", tfidf_combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_scores = tfidf_context_df.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# Plot top 20 words by TF-IDF score\n",
    "top_n = 20\n",
    "top_words = word_scores.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = plt.cm.tab20.colors  # Use a colormap for different colors\n",
    "top_words.plot(kind='bar', color=[colors[i % len(colors)] for i in range(len(top_words))])\n",
    "plt.title(f\"Top {top_n} Words by TF-IDF Score\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Total TF-IDF Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a column to the DataFrame with the top N words for each document\n",
    "feature_names = vectorizer_context.get_feature_names_out()\n",
    "\n",
    "# Function to get top N words for a single document\n",
    "def get_top_n_words(row, n):\n",
    "    row_data = row.toarray().flatten()\n",
    "    top_indices = row_data.argsort()[::-1][:n]\n",
    "    \n",
    "    return [(feature_names[i], row_data[i]) for i in top_indices if row_data[i] > 0]\n",
    "\n",
    "\n",
    "# Apply the function to each row of the TF-IDF DataFrame\n",
    "N = 5 # Number of top words to extract\n",
    "#Next to the word there is also the tfidf score\n",
    "train_df_tokenized[f'top_{N}_tfidf_words_context'] = [\n",
    "    get_top_n_words(tfidf_context[i], N) for i in range(tfidf_context.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fafac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_tokenized.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import to_hex\n",
    "import random\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def highlight_words_in_text_colored(text, words_to_highlight, colors):\n",
    "    \"\"\"\n",
    "    Highlight specific words in a given text with corresponding colors.\n",
    "    \"\"\"\n",
    "    for (word, _), color in zip(words_to_highlight, colors):\n",
    "        # Use regex to match whole words only\n",
    "        text = re.sub(rf'\\b{re.escape(word)}\\b', f\"<mark style='background-color:{color};'>{word}</mark>\", text)\n",
    "    return text\n",
    "\n",
    "def plot_and_highlight_document_colored(doc_index, title='Top TF-IDF Words'):\n",
    "    \"\"\"\n",
    "    Plot top TF-IDF words with different colors and highlight them in the document with the same colors.\n",
    "    \"\"\"\n",
    "    word_score_pairs = train_df_tokenized.loc[doc_index, f'top_{N}_tfidf_words_context']\n",
    "    words, scores = zip(*word_score_pairs)\n",
    "    \n",
    "    # Generate unique colors for each word\n",
    "    colors = [to_hex(plt.cm.tab10(i % 10)) for i in range(len(words))]\n",
    "    \n",
    "    # Plot top TF-IDF words with different colors\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(words[::-1], scores[::-1], color=colors[::-1])  # reverse for descending order\n",
    "    plt.xlabel('TF-IDF Score')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Highlight words in the document with the same colors\n",
    "    highlighted_text = highlight_words_in_text_colored(train_df_tokenized.loc[doc_index, 'context_str'], word_score_pairs, colors)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.axis('off')\n",
    "    display(HTML(f\"<div style='font-size:14px; line-height:1.6;'>{highlighted_text}</div>\"))\n",
    "    plt.show()\n",
    "\n",
    "# Example: visualize the first 5 documents\n",
    "\n",
    "documents_to_show = 3\n",
    "for i in range(3):\n",
    "    plot_and_highlight_document_colored(i, title=f'Document {i+1} - Top TF-IDF Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ceaea",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è Dataset indexing & searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42867df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Create the dictionary and corpus for the TF-IDF model\n",
    "texts = train_df['context']\n",
    "texts = [word_tokenize(text.lower()) for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#¬†Build the TF-IDF vectorizer\n",
    "tfidf_model = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "\n",
    "# Create the similarity index\n",
    "index = similarities.MatrixSimilarity(corpus_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d1cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Function to get the most similar documents\n",
    "def search(query_tokens, top_k=3):\n",
    "    query_bow = dictionary.doc2bow(query_tokens)\n",
    "    query_tfidf = tfidf_model[query_bow]\n",
    "    sims = index[query_tfidf]  # cosine similarities\n",
    "    top_indices = sorted(enumerate(sims), key=lambda x: -x[1])[:top_k]\n",
    "    results = train_df.iloc[[i for i, _ in top_indices]].copy()\n",
    "    results['similarity'] = [score for _, score in top_indices]\n",
    "    return results[['context', 'question', 'answer', 'similarity']]\n",
    "\n",
    "# Create an output widget for displaying the results\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Function to handle search queries\n",
    "def interactive_search(change):\n",
    "    query = change['new']  \n",
    "    query_tokens = preprocess(query)  # Tokenize and preprocess the query\n",
    "    result_df = search(query_tokens)\n",
    "    with output_area:\n",
    "        clear_output(wait=True)  # Clear only the output area\n",
    "        output_area.append_display_data(result_df)  # Append the resulting dataframe to the output area\n",
    "\n",
    "# Create a text input widget\n",
    "search_bar = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your search query',\n",
    "    description='Search:',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Add an observer to the search bar\n",
    "search_bar.observe(interactive_search, names='value')\n",
    "\n",
    "# Display the search bar and output area\n",
    "display(search_bar, output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050162dd",
   "metadata": {},
   "source": [
    "# üìù Sentence embedding (all-MiniLM-L6-v2 on context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Model for computing sentence embeddings. \n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentence_embeddings = model.encode(train_df['context'].tolist(), convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d7d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform community detection on the sentence embeddings\n",
    "clusters = util.community_detection(sentence_embeddings, min_community_size=10, threshold=0.6, show_progress_bar=True)\n",
    "\n",
    "print(\"Number of clusters found:\", len(clusters))\n",
    "cluster_sizes = [len(cluster) for cluster in clusters]\n",
    "print(\"Cluster coverage:\", sum(cluster_sizes) / len(train_df))\n",
    "\n",
    "# Plot the distribution of cluster sizes\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(cluster_sizes, kde=True)\n",
    "plt.title('Distribution of Cluster Sizes')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a68b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print all clusters with the members\n",
    "# for i, cluster in enumerate(clusters):\n",
    "#     print(\"\\nCluster {}, #{} Elements \".format(i+1, len(cluster)))\n",
    "#     for sentence_id in cluster:\n",
    "#         print(\"\\t\", train_df['context'].iloc[sentence_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b53e63",
   "metadata": {},
   "source": [
    "# üé© Topic Modelling (BERTopic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c79fc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Create a BERTopic model\n",
    "topic_model = BERTopic(verbose=True)\n",
    "topics, probs = topic_model.fit_transform(train_df['context'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d873c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of topics found:\", len(topic_model.get_topic_info()))\n",
    "# Display the topics\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4718753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Visualize the topics\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b36d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the document representation\n",
    "topic_model.visualize_documents(train_df['context'].tolist(), topics = list(range(20)), hide_document_hover=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ddc71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to incompatibility with the current environment, we will not visualize the document datamap if using Kaggle\n",
    "if not IN_KAGGLE:\n",
    "  # Visualize the document datamap\n",
    "  topic_model.visualize_document_datamap(train_df['context'].tolist(), interactive=True, enable_search=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c06cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the topic representation\n",
    "topic_model.visualize_barchart(top_n_topics=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_no = 27\n",
    "\n",
    "print(f\"Topic {topic_no}:\")\n",
    "# get the topic representation\n",
    "for topic, prob in topic_model.get_topic(topic_no):\n",
    "    print(f\"{topic}: {prob:.2f}\")\n",
    "# find all the documents in a topic and relative topics in a pretty format\n",
    "for i in topic_model.get_representative_docs(topic_no):\n",
    "\n",
    "    index = train_df[train_df['context'].str.contains(i)].index[0]\n",
    "    print(f'Document {index}\\nContext: {i}')\n",
    "    # find the record in the original dataset by a serach on the context and print question and answer\n",
    "    print(\"Question:\", train_df[train_df['context'].str.contains(i)]['question'].values[0])\n",
    "    print(\"Answer:\", train_df[train_df['context'].str.contains(i)]['answer'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184b74ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sviualize the word topics without the document representation\n",
    "topic_model.visualize_heatmap(top_n_topics=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd876031",
   "metadata": {},
   "source": [
    "## Hierarchical clustering on topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(train_df['context'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19007a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad74826",
   "metadata": {},
   "outputs": [],
   "source": [
    "itables.show(hierarchical_topics.sort_values(by='Distance', ascending=False), max_rows=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f24240",
   "metadata": {},
   "source": [
    "# üîÆ Microsoft phi-2 LLM model QA ability testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b9ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_KAGGLE:\n",
    "    torch.set_default_device(\"cuda\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/phi-2\", \n",
    "    torch_dtype=torch.float16, \n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11273c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 5\n",
    "\n",
    "train_df = dataset['train'].to_pandas()\n",
    "\n",
    "subset = train_df.sample(n_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c72464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_answer(entry, generation_function, context=False):\n",
    "    question = entry['question']\n",
    "    context = entry['context']\n",
    "    if context:\n",
    "        answer = generation_function(question, context)\n",
    "    else:\n",
    "        answer = generation_function(question)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4624ae1",
   "metadata": {},
   "source": [
    "## Zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_zeroshot(entry):\n",
    "    question = entry['question']\n",
    "    prompt = f'''\n",
    "        You are an assistant designed to answer questions.\n",
    "        The question will be provided at the end of the prompt.\n",
    "        Just answer the question concisely without any further text.\n",
    "        Question: {question}\n",
    "        '''\n",
    "    inputs = tokenizer(\n",
    "        question, \n",
    "        return_tensors=\"pt\", \n",
    "        return_attention_mask=False\n",
    "    )\n",
    "    outputs = model.generate(**inputs, max_length=1024)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec544f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"Generating answers using zero-shot phi-2\")\n",
    "subset['zeroShot_answer'] = subset.progress_apply(generate_answer_zeroshot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset[['question', 'answer', 'zeroShot_answer']].head(n_test)\n",
    "# Save the subset with generated answers to a CSV file\n",
    "subset.to_csv('subset_with_generated_answers.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d722ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_zeroshot_context(question, context):\n",
    "    prompt = f'''\n",
    "        You are an assistant designed to answer questions.\n",
    "        The context and question will be provided below.\n",
    "        Use the context to answer the question concisely without any further text.\n",
    "        Context: {context}\n",
    "        Question: {question}\n",
    "        '''\n",
    "    inputs = tokenizer(\n",
    "        question, \n",
    "        return_tensors=\"pt\", \n",
    "        return_attention_mask=False\n",
    "    )\n",
    "    outputs = model.generate(**inputs, max_length=1024)\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"Generating answers using zero-shot with context phi-2\")\n",
    "subset['zeroShot_context_answer'] = subset.progress_apply(store_answer(generate_answer_zeroshot_context,context=True), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
