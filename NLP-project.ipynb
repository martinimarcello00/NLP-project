{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7914bf7",
   "metadata": {
    "id": "c7914bf7"
   },
   "source": [
    "# NLP Project\n",
    "\n",
    "*Team members*:\n",
    "- Asja Attanasio\n",
    "- Daniele Lagan√†\n",
    "- Marcello Martini\n",
    "- Gianluigi Palmisano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b74fcf",
   "metadata": {
    "id": "38b74fcf"
   },
   "source": [
    "# ‚¨ÜÔ∏è Dataset import and library setup\n",
    "\n",
    "In this section, we import all the necessary Python libraries to support data loading, preprocessing, analysis, and visualization. \n",
    "\n",
    "The functionalities of the imported libraries include:\n",
    "- **Data manipulation**: `pandas`, `numpy`, `collections.Counter`\n",
    "- **Text processing and analysis**: `nltk` for tokenization, stopword removal, and frequency distribution\n",
    "- **Data visualization**: `matplotlib.pyplot` and `seaborn` for plotting and visual exploration\n",
    "- **Transformer models and training**: `transformers` for loading pretrained tokenizers and models, and for training with `Trainer` and `TrainingArguments`\n",
    "- **Dataset management**: `datasets` from Hugging Face for loading and working with NLP datasets\n",
    "- **Utility tools**: `tqdm` for progress bars, `re` for regular expressions, and `torch` for PyTorch operations\n",
    "- **Interactive table display**: `itables` to enable interactive views of `pandas` DataFrames in the notebook\n",
    "\n",
    "We then load the dataset from the Hugging Face Hub: [neural-bridge/rag-dataset-12000](https://huggingface.co/datasets/neural-bridge/rag-dataset-12000). \n",
    "\n",
    "Each sample consists of:\n",
    "- `context`: a passage containing the information needed to answer the question\n",
    "- `question`: a natural language question related to the context\n",
    "- `answer`: a response automatically generated by GPT-4\n",
    "\n",
    "The dataset is already divided in two sets:\n",
    "- **Train set**: 9,600 samples\n",
    "- **Test set**: 2,400 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914afcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:02:25.487453Z",
     "iopub.status.busy": "2025-05-08T10:02:25.487205Z",
     "iopub.status.idle": "2025-05-08T10:02:35.874852Z",
     "shell.execute_reply": "2025-05-08T10:02:35.874140Z",
     "shell.execute_reply.started": "2025-05-08T10:02:25.487432Z"
    },
    "id": "1914afcd",
    "outputId": "756dd0b6-8575-4fb8-fc8a-03ceefa2f0c6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "# Check if running in Kaggle\n",
    "IN_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
    "\n",
    "if IN_KAGGLE:\n",
    "  os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "  from plotly.offline import init_notebook_mode\n",
    "  init_notebook_mode(connected=True)\n",
    "\n",
    "if IN_COLAB:\n",
    "  !pip install datasets gensim datamapplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e57992b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:02:35.877314Z",
     "iopub.status.busy": "2025-05-08T10:02:35.877061Z",
     "iopub.status.idle": "2025-05-08T10:02:39.494371Z",
     "shell.execute_reply": "2025-05-08T10:02:39.493773Z",
     "shell.execute_reply.started": "2025-05-08T10:02:35.877292Z"
    },
    "id": "6e57992b",
    "outputId": "208a073c-3b14-49f9-eea0-87e5e991bb8a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import itables\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# For interactive tables\n",
    "itables.init_notebook_mode(all_interactive=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769dedbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:02:39.495360Z",
     "iopub.status.busy": "2025-05-08T10:02:39.495021Z",
     "iopub.status.idle": "2025-05-08T10:02:41.749433Z",
     "shell.execute_reply": "2025-05-08T10:02:41.748903Z",
     "shell.execute_reply.started": "2025-05-08T10:02:39.495342Z"
    },
    "id": "769dedbd",
    "outputId": "44f356da-34ec-4194-fa0f-b68919a61119",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import dataset using the Hugging Face datasets library\n",
    "dataset = load_dataset(\"neural-bridge/rag-dataset-12000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2cd6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:02:41.750346Z",
     "iopub.status.busy": "2025-05-08T10:02:41.750113Z",
     "iopub.status.idle": "2025-05-08T10:02:41.755384Z",
     "shell.execute_reply": "2025-05-08T10:02:41.754671Z",
     "shell.execute_reply.started": "2025-05-08T10:02:41.750329Z"
    },
    "id": "0bf2cd6a",
    "outputId": "4f625c26-9818-49f2-df1a-5a350a80d632",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6b773",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:02:41.756294Z",
     "iopub.status.busy": "2025-05-08T10:02:41.756046Z",
     "iopub.status.idle": "2025-05-08T10:02:41.898649Z",
     "shell.execute_reply": "2025-05-08T10:02:41.898076Z",
     "shell.execute_reply.started": "2025-05-08T10:02:41.756273Z"
    },
    "id": "1ed6b773",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = dataset['train'].to_pandas()\n",
    "test_df = dataset['test'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb63d3a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:02:41.899604Z",
     "iopub.status.busy": "2025-05-08T10:02:41.899400Z",
     "iopub.status.idle": "2025-05-08T10:02:41.919887Z",
     "shell.execute_reply": "2025-05-08T10:02:41.919366Z",
     "shell.execute_reply.started": "2025-05-08T10:02:41.899587Z"
    },
    "id": "abb63d3a",
    "outputId": "df8b3ffe-4bcf-4390-e557-84458798aa2c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23eb9ff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:02:41.921261Z",
     "iopub.status.busy": "2025-05-08T10:02:41.920537Z",
     "iopub.status.idle": "2025-05-08T10:02:41.926356Z",
     "shell.execute_reply": "2025-05-08T10:02:41.925675Z",
     "shell.execute_reply.started": "2025-05-08T10:02:41.921232Z"
    },
    "id": "f23eb9ff",
    "outputId": "40169120-395e-48ba-948e-d4b289375f5c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Train set size:\", len(train_df))\n",
    "print(\"Test set size:\", len(test_df))\n",
    "print(\"Columns:\", train_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df309894",
   "metadata": {
    "id": "df309894"
   },
   "source": [
    "# üîé Data exploration\n",
    "\n",
    "In this section, we perform an initial exploration of the dataset to better understand its structure, content, and quality. The main objectives of this phase are to clean the data, analyze basic textual properties, and identify key patterns that may inform later modeling decisions.\n",
    "\n",
    "We begin by identifying and removing rows with missing values in the `context`, `question`, or `answer` fields. Texts are then preprocessed using NLTK: tokenized, lowercased, stripped of punctuation, and filtered for stopwords.\n",
    "\n",
    "We compute token length statistics for `context` and `question` fields and build a vocabulary from the training set, then calculate the frequency distribution of all tokens in the training data and visualize it.\n",
    "\n",
    "Finally, we analyze the frequency of interrogative words (`what`, `where`, `when`, `who`, `why`, `how`, `which`) in the questions to gain insight into their nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7ac1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows with at least one missing value in 'context', 'question', or 'answer'\n",
    "missing_rows_train = train_df[['context', 'question', 'answer']].isnull().any(axis=1).sum()\n",
    "missing_rows_test = test_df[['context', 'question', 'answer']].isnull().any(axis=1).sum()\n",
    "\n",
    "print(\"Rows with at least one missing value in train set:\", missing_rows_train)\n",
    "print(\"Rows with at least one missing value in test set:\", missing_rows_test)\n",
    "\n",
    "# Remove null values\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e862d356",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 583
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:02:41.954747Z",
     "iopub.status.busy": "2025-05-08T10:02:41.954468Z",
     "iopub.status.idle": "2025-05-08T10:03:18.131387Z",
     "shell.execute_reply": "2025-05-08T10:03:18.130624Z",
     "shell.execute_reply.started": "2025-05-08T10:02:41.954727Z"
    },
    "id": "e862d356",
    "outputId": "dc17fb0a-fefd-4d9e-80f1-06bacfaccf2d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "# Initialize stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize lists and variables for analysis\n",
    "context_lengths = []\n",
    "question_lengths = []\n",
    "vocab = set()\n",
    "\n",
    "# Initialize lists for all tokens\n",
    "all_tokens = []\n",
    "train_df_tokenized = []\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "    \"\"\"Tokenize, lowercase, remove punctuation and stopwords from text.\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum()]  # Keep only alphanumeric tokens\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return tokens\n",
    "\n",
    "# Function to process a single entry\n",
    "def process_entry(entry):\n",
    "    \"\"\"Process a single dataset entry to extract tokens and update statistics.\"\"\"\n",
    "    context_tokens = preprocess(entry['context'])\n",
    "    question_tokens = preprocess(entry['question'])\n",
    "\n",
    "    # Update lengths\n",
    "    context_lengths.append(len(context_tokens))\n",
    "    question_lengths.append(len(question_tokens))\n",
    "    answer_tokens = preprocess(entry['answer'])\n",
    "\n",
    "    # Update vocabulary and token list\n",
    "    vocab.update(context_tokens)\n",
    "    vocab.update(question_tokens)\n",
    "    all_tokens.extend(context_tokens)\n",
    "    all_tokens.extend(question_tokens)\n",
    "    train_df_tokenized.append({'context': context_tokens, 'question': question_tokens, 'answer': answer_tokens})\n",
    "\n",
    "tqdm.pandas(desc=\"Processing entries\")\n",
    "train_df.progress_apply(process_entry, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37b4d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:03:18.132291Z",
     "iopub.status.busy": "2025-05-08T10:03:18.132075Z",
     "iopub.status.idle": "2025-05-08T10:03:19.495948Z",
     "shell.execute_reply": "2025-05-08T10:03:19.495396Z",
     "shell.execute_reply.started": "2025-05-08T10:03:18.132275Z"
    },
    "id": "1b37b4d1",
    "outputId": "ca76fc35-894d-4f23-93cf-561e93974fbc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compute the frequency distribution of tokens\n",
    "fdist = FreqDist(all_tokens)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2db549",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:03:19.496929Z",
     "iopub.status.busy": "2025-05-08T10:03:19.496729Z",
     "iopub.status.idle": "2025-05-08T10:03:20.653395Z",
     "shell.execute_reply": "2025-05-08T10:03:20.652717Z",
     "shell.execute_reply.started": "2025-05-08T10:03:19.496914Z"
    },
    "id": "ff2db549",
    "outputId": "2d18b597-1cf7-46a5-cfb4-09f7a0e91781",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Plot the word cloud\n",
    "wordcloud = WordCloud(width=800, height=500).generate_from_frequencies(fdist)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud of Vocabulary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8f154",
   "metadata": {},
   "source": [
    "The word cloud confirms that the dataset is well-suited for training models on general-domain retrieval-based QA. The shared vocabulary between context and question tokens implies:\n",
    "\n",
    "- Strong lexical overlap between what is asked and what is retrieved,\n",
    "\n",
    "- Good support for tasks requiring semantic matching and answer extraction,\n",
    "\n",
    "- A balanced mix of common factual knowledge, reasoning, and information-seeking behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71689608",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:03:20.655321Z",
     "iopub.status.busy": "2025-05-08T10:03:20.654248Z",
     "iopub.status.idle": "2025-05-08T10:03:20.949423Z",
     "shell.execute_reply": "2025-05-08T10:03:20.948656Z",
     "shell.execute_reply.started": "2025-05-08T10:03:20.655297Z"
    },
    "id": "71689608",
    "outputId": "06885f6c-2c88-4cbf-cd0c-e5577daf3195",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot the frequency distribution of the most common words\n",
    "fdist.plot(30,  title=\"Top 30 Most Frequent Words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792d4b8",
   "metadata": {},
   "source": [
    "This frequency plot quantitatively supports the earlier word cloud insights by highlighting the most dominant tokens in the combined context and question texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4b44a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:03:20.950471Z",
     "iopub.status.busy": "2025-05-08T10:03:20.950179Z",
     "iopub.status.idle": "2025-05-08T10:03:21.213442Z",
     "shell.execute_reply": "2025-05-08T10:03:21.212803Z",
     "shell.execute_reply.started": "2025-05-08T10:03:20.950447Z"
    },
    "id": "e0c4b44a",
    "outputId": "53d5b715-51bc-429f-f2a5-e0e0828b7a2e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Print statistics\n",
    "print(f\"Number of documents: {len(train_df)}\")\n",
    "print(f\"Average context length: {sum(context_lengths)/len(context_lengths):.2f} tokens\")\n",
    "print(f\"Average question length: {sum(question_lengths)/len(question_lengths):.2f} tokens\")\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f104d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of context lengths\n",
    "sns.histplot(context_lengths, bins=50, kde=True)\n",
    "plt.title('Distribution of Context Lengths')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21458b6b",
   "metadata": {},
   "source": [
    "Most contexts fall between 200 and 450 tokens, peaking around 300‚Äì350 tokens. This suggests that the dataset emphasizes substantial, information-rich contexts, making it well-suited for tasks like retrieval-augmented generation.\n",
    "\n",
    "The distribution is right-skewed, with fewer very long contexts (up to 1000+ tokens), but they are still meaningfully present. In fact, even if such long contexts make up a small percentage of the dataset, they can be important for training robustness.\n",
    "This indicates a diversity of passage lengths, likely sourced from a variety of materials, from short summaries to full paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e5f719",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:03:21.214351Z",
     "iopub.status.busy": "2025-05-08T10:03:21.214128Z",
     "iopub.status.idle": "2025-05-08T10:03:21.460457Z",
     "shell.execute_reply": "2025-05-08T10:03:21.459744Z",
     "shell.execute_reply.started": "2025-05-08T10:03:21.214335Z"
    },
    "id": "c5e5f719",
    "outputId": "2f1e77ec-118f-40a8-8928-82173b575710",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(question_lengths, bins=50, kde=True)\n",
    "plt.title(\"Question Length Distribution\")\n",
    "plt.xlabel(\"Number of Tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc1b30",
   "metadata": {},
   "source": [
    "We see that the questions in this dataset tend to be quite concise. The histogram peaks sharply around 6 tokens, indicating that this is the most common question length. In fact, the majority of the dataset appears to be concentrated between 4 and 10 tokens, suggesting that the dataset favors short, focused queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b95914d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:03:21.461510Z",
     "iopub.status.busy": "2025-05-08T10:03:21.461288Z",
     "iopub.status.idle": "2025-05-08T10:03:22.335855Z",
     "shell.execute_reply": "2025-05-08T10:03:22.335197Z",
     "shell.execute_reply.started": "2025-05-08T10:03:21.461496Z"
    },
    "id": "1b95914d",
    "outputId": "ea748109-130e-4b54-8d88-d434bcfa40b2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Show the distribttion of the question words\n",
    "\n",
    "question_words = [\"what\", \"where\", \"when\", \"why\", \"how\", \"who\", \"which\"]\n",
    "question_word_counts = {word: 0 for word in question_words}\n",
    "for question in train_df['question']:\n",
    "    tokens = word_tokenize(question.lower())\n",
    "    for word in question_words:\n",
    "        question_word_counts[word] += tokens.count(word)\n",
    "question_word_counts\n",
    "question_word_counts_df = pd.DataFrame(list(question_word_counts.items()), columns=['Question Word', 'Count'])\n",
    "question_word_counts_df = question_word_counts_df.sort_values(by='Count', ascending=False)\n",
    "# Plotting the counts of question words\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Question Word', y='Count', data=question_word_counts_df)\n",
    "plt.title('Counts of Question Words')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa9cb2",
   "metadata": {
    "id": "3daa9cb2"
   },
   "source": [
    "# üßπ Data Cleaning\n",
    "\n",
    "In this step, we clean and standardize the text data across the `context`, `question`, and `answer` fields. All text is converted to lowercase, tokenized, lemmatized, and stripped of punctuation and stopwords. For the `question` field, key interrogative words (e.g., *what*, *how*, *why*) are preserved to retain the original intent of the queries. The cleaning process is applied to both training and test sets, with progress tracked using `tqdm`. This results in a consistent, noise-reduced dataset ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15417845",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:03:22.336821Z",
     "iopub.status.busy": "2025-05-08T10:03:22.336575Z",
     "iopub.status.idle": "2025-05-08T10:03:22.344087Z",
     "shell.execute_reply": "2025-05-08T10:03:22.343389Z",
     "shell.execute_reply.started": "2025-05-08T10:03:22.336795Z"
    },
    "id": "15417845",
    "outputId": "d244035d-a22e-41cc-c8ae-bed9af82f5f4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# first entry in train dataset as table without truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400877f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:03:22.345032Z",
     "iopub.status.busy": "2025-05-08T10:03:22.344848Z",
     "iopub.status.idle": "2025-05-08T10:03:22.409748Z",
     "shell.execute_reply": "2025-05-08T10:03:22.409034Z",
     "shell.execute_reply.started": "2025-05-08T10:03:22.345019Z"
    },
    "id": "400877f7",
    "outputId": "0e82e18f-e3bb-4ec2-ffb0-5b11b0c24eb5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text, preserve_question_words=False):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text by removing punctuation, stopwords, and optionally preserving question words.\n",
    "    \"\"\"\n",
    "    # Define question words to preserve\n",
    "    question_words = {\"what\", \"where\", \"when\", \"why\", \"how\", \"who\", \"which\"}\n",
    "\n",
    "    # Replace newline characters with a space\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # Tokenize and lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Lemmatize, remove punctuation, and stopwords\n",
    "    if preserve_question_words:\n",
    "        # Preserve question words\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and (word not in stop_words or word in question_words)]\n",
    "    else:\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e9e615",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-08T10:03:22.410632Z",
     "iopub.status.busy": "2025-05-08T10:03:22.410456Z",
     "iopub.status.idle": "2025-05-08T10:04:23.162720Z",
     "shell.execute_reply": "2025-05-08T10:04:23.162136Z",
     "shell.execute_reply.started": "2025-05-08T10:03:22.410619Z"
    },
    "id": "26e9e615",
    "outputId": "537bcd52-96e5-4c79-fc26-8f8195408c8d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize tqdm for pandas to show progress\n",
    "tqdm.pandas(desc=\"Cleaning Text\")\n",
    "\n",
    "# Clean the training set\n",
    "train_df['context'] = train_df['context'].progress_apply(lambda x: clean_text(x))\n",
    "train_df['question'] = train_df['question'].progress_apply(lambda x: clean_text(x, preserve_question_words=True))\n",
    "train_df['answer'] = train_df['answer'].progress_apply(lambda x: clean_text(x))\n",
    "\n",
    "# Clean the test set\n",
    "test_df['context'] = test_df['context'].progress_apply(lambda x: clean_text(x))\n",
    "test_df['question'] = test_df['question'].progress_apply(lambda x: clean_text(x, preserve_question_words=True))\n",
    "test_df['answer'] = test_df['answer'].progress_apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29fc90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:04:23.163796Z",
     "iopub.status.busy": "2025-05-08T10:04:23.163506Z",
     "iopub.status.idle": "2025-05-08T10:04:23.171115Z",
     "shell.execute_reply": "2025-05-08T10:04:23.170447Z",
     "shell.execute_reply.started": "2025-05-08T10:04:23.163773Z"
    },
    "id": "cf29fc90",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578a3038",
   "metadata": {
    "id": "578a3038"
   },
   "source": [
    "# üí¨ Word Embedding (Word2Vec - questions)\n",
    "\n",
    "In this section the main goal was to first tokenize the questions into words, create their embeddings and finally to cluster them to find the best groups of words.\n",
    "To do so we used the following:\n",
    "\n",
    "- **Word Embeddings**: We implemented Word2Vec using skip-gram model with negative sampling to represent words in 5-dimensional vector space. This approach captures semantic relationships between words in questions, where words with similar meanings have vectors that are closer together.\n",
    "\n",
    "- **Clustering**: K-means clustering with 4 clusters was applied to group similar questions together based on their average word embeddings. We visualized these clusters using both t-SNE for 2D representation and PCA for 3D interactive visualization. To determine that 4 was the optimal number of clusters, we performed elbow analysis examining the trade-off between cluster compactness and separation.\n",
    "\n",
    "- **Scoring metrics**: We evaluated cluster quality using silhouette scores, Within-Cluster Sum of Squares (WCSS), and Between-Cluster Sum of Squares (BSS). These metrics helped us determine the optimal number of clusters by analyzing both the compactness within clusters and separation between clusters.\n",
    "\n",
    "The clustering revealed distinct question types, with each cluster characterized by frequently occurring words that suggest the semantic nature of those questions. This approach allowed us to gain insights into the different types of questions in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df35810",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:04:23.172174Z",
     "iopub.status.busy": "2025-05-08T10:04:23.171957Z",
     "iopub.status.idle": "2025-05-08T10:05:08.176623Z",
     "shell.execute_reply": "2025-05-08T10:05:08.176035Z",
     "shell.execute_reply.started": "2025-05-08T10:04:23.172159Z"
    },
    "id": "3df35810",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize the questions into words\n",
    "train_df['tokenized_question'] = train_df['question'].apply(lambda x: word_tokenize(x.lower()))\n",
    "\n",
    "vector_size = 5 # Size of the word vectors\n",
    "\n",
    "# Train the Word2Vec model with optimized parameters for better embedding quality\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=train_df['tokenized_question'],\n",
    "    vector_size=vector_size,\n",
    "    window=10,  # Increase the context window size\n",
    "    min_count=2,  # Ignore words that appear less frequently\n",
    "    workers=4,\n",
    "    sg=1,  # Use skip-gram model for better performance on smaller datasets\n",
    "    hs=0,  # Use negative sampling\n",
    "    negative=10,  # Number of negative samples\n",
    "    epochs=20  # Train for more epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ebcf47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:05:08.177948Z",
     "iopub.status.busy": "2025-05-08T10:05:08.177455Z",
     "iopub.status.idle": "2025-05-08T10:05:08.409421Z",
     "shell.execute_reply": "2025-05-08T10:05:08.408666Z",
     "shell.execute_reply.started": "2025-05-08T10:05:08.177929Z"
    },
    "id": "35ebcf47",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to get the average word embedding for a question\n",
    "def get_average_embedding(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)  # Return a zero vector if no words are found\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Generate sentence embeddings for the questions\n",
    "train_df['question_embedding'] = train_df['tokenized_question'].apply(lambda x: get_average_embedding(x, word2vec_model))\n",
    "\n",
    "# Stack embeddings into a matrix\n",
    "question_embeddings = np.stack(train_df['question_embedding'].values)\n",
    "question_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad52582",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:05:08.410323Z",
     "iopub.status.busy": "2025-05-08T10:05:08.410114Z",
     "iopub.status.idle": "2025-05-08T10:05:38.313045Z",
     "shell.execute_reply": "2025-05-08T10:05:38.312287Z",
     "shell.execute_reply.started": "2025-05-08T10:05:08.410308Z"
    },
    "id": "aad52582",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Function to find the optimal number of clusters using the Elbow Method and Silhouette Analysis\n",
    "def find_optimal_clusters(embeddings, max_clusters=10):\n",
    "    wcss = []  # Within-Cluster Sum of Squares\n",
    "    bss = []  # Between-Cluster Sum of Squares\n",
    "    silhouette_scores = []  # Silhouette Scores\n",
    "    cluster_range = range(2, max_clusters + 1)\n",
    "\n",
    "    # Calculate Total Sum of Squares (TSS)\n",
    "    tss = np.sum((embeddings - np.mean(embeddings, axis=0))**2)\n",
    "\n",
    "    for n_clusters in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        kmeans.fit(embeddings)\n",
    "\n",
    "        # Calculate WCSS\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "        # Calculate BSS\n",
    "        bss.append(tss - kmeans.inertia_)\n",
    "\n",
    "        # Calculate Silhouette Score\n",
    "        silhouette_avg = silhouette_score(embeddings, kmeans.labels_)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Plot Elbow Method for WCSS and BSS\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cluster_range, wcss, marker='o', label='WCSS (Within-Cluster Sum of Squares)')\n",
    "    plt.plot(cluster_range, bss, marker='o', label='BSS (Between-Cluster Sum of Squares)')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Sum of Squares')\n",
    "    plt.title('Elbow Method for Optimal Clusters')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Silhouette Scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(cluster_range, silhouette_scores, marker='o', label='Silhouette Score')\n",
    "    # Plot the cluster number above each point\n",
    "    for i, score in enumerate(silhouette_scores):\n",
    "        plt.text(cluster_range[i], score, str(cluster_range[i]), fontsize=9, ha='center')\n",
    "    plt.xlabel('Number of Clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title('Silhouette Analysis for Optimal Clusters')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return wcss, bss, silhouette_scores\n",
    "\n",
    "# Find the optimal number of clusters\n",
    "wcss, bss, silhouette_scores = find_optimal_clusters(question_embeddings, max_clusters=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271e9faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:05:38.314253Z",
     "iopub.status.busy": "2025-05-08T10:05:38.313952Z",
     "iopub.status.idle": "2025-05-08T10:06:20.329676Z",
     "shell.execute_reply": "2025-05-08T10:06:20.329054Z",
     "shell.execute_reply.started": "2025-05-08T10:05:38.314230Z"
    },
    "id": "271e9faf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "n_clusters = 4\n",
    "\n",
    "# Perform K-Means clustering with n_clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "train_df['cluster'] = kmeans.fit_predict(question_embeddings)\n",
    "\n",
    "# Reduce dimensions of question embeddings using t-SNE for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "question_embeddings_2d = tsne.fit_transform(question_embeddings)\n",
    "\n",
    "# Plot the t-SNE results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(question_embeddings_2d[:, 0], question_embeddings_2d[:, 1], c=train_df['cluster'], cmap='viridis', s=10)\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.title(f't-SNE Visualization of Question Clusters ({n_clusters} Clusters)')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.show()\n",
    "\n",
    "# Function to extract topics for each cluster\n",
    "def get_cluster_topics(df, cluster_col, text_col, top_n=10):\n",
    "    cluster_topics = {}\n",
    "    for cluster_id, group in df.groupby(cluster_col):\n",
    "        # Tokenize all questions in the cluster\n",
    "        all_words = [word for question in group[text_col] for word in word_tokenize(question.lower())]\n",
    "        # Count word frequencies\n",
    "        word_freq = Counter(all_words)\n",
    "        # Get the top N most common words\n",
    "        cluster_topics[cluster_id] = word_freq.most_common(top_n)\n",
    "    return cluster_topics\n",
    "\n",
    "# Get topics for each cluster\n",
    "topics = get_cluster_topics(train_df, cluster_col='cluster', text_col='question', top_n=10)\n",
    "\n",
    "# Display topics for each cluster\n",
    "for cluster_id, words in topics.items():\n",
    "    print(f\"Cluster {cluster_id}:\")\n",
    "    print(\", \".join([f\"{word} ({count})\" for word, count in words]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d94cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:06:20.330656Z",
     "iopub.status.busy": "2025-05-08T10:06:20.330438Z",
     "iopub.status.idle": "2025-05-08T10:06:23.081501Z",
     "shell.execute_reply": "2025-05-08T10:06:23.080525Z",
     "shell.execute_reply.started": "2025-05-08T10:06:20.330639Z"
    },
    "id": "259d94cd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "# Reduce dimensions of question embeddings to 3D using PCA\n",
    "pca = PCA(n_components=3)\n",
    "question_embeddings_3d = pca.fit_transform(question_embeddings)\n",
    "\n",
    "\n",
    "# Prepare a DataFrame for visualization\n",
    "visualization_df = pd.DataFrame({\n",
    "    'PCA Dimension 1': question_embeddings_3d[:, 0],\n",
    "    'PCA Dimension 2': question_embeddings_3d[:, 1],\n",
    "    'PCA Dimension 3': question_embeddings_3d[:, 2],\n",
    "    'Cluster': train_df['cluster'],\n",
    "    'Question': train_df['question']\n",
    "})\n",
    "# Create an interactive 3D scatter plot with a larger figure size\n",
    "fig = px.scatter_3d(\n",
    "    visualization_df,\n",
    "    x='PCA Dimension 1',\n",
    "    y='PCA Dimension 2',\n",
    "    z='PCA Dimension 3',\n",
    "    color='Cluster',\n",
    "    hover_data=['Question'],  # Show the question on hover\n",
    "    title='Interactive 3D Visualization of Question Clusters',\n",
    "    width=1200,  # Set the width of the plot\n",
    "    height=1000   # Set the height of the plot\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac16123",
   "metadata": {
    "id": "6ac16123"
   },
   "source": [
    "# üî† TF-IDF analysis\n",
    "\n",
    "In this section we performed TF-IDF analysis to extract the most important words from documents in our dataset. TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "The TF-IDF score is calculated using the formula:\n",
    "\n",
    "$$\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)$$\n",
    "\n",
    "Where:\n",
    "- $\\text{TF}(t, d)$ is the term frequency of term $t$ in document $d$ (how many times the term appears in the document)\n",
    "- $\\text{IDF}(t, D)$ is the inverse document frequency of term $t$ in document collection $D$, calculated as $\\log\\frac{N}{df_t}$ where $N$ is the total number of documents and $df_t$ is the number of documents containing term $t$\n",
    "\n",
    "We applied TF-IDF analysis in three different ways:\n",
    "1. Context-only analysis: Applied TF-IDF on just the context fields\n",
    "2. Question-only analysis: Applied TF-IDF on just the question fields\n",
    "3. Combined analysis: Applied TF-IDF on the combined context and question fields\n",
    "\n",
    "For each document, we extracted the top 5 words with the highest TF-IDF scores, indicating the most distinctive terms for that specific document. These terms represent the key concepts that distinguish each document from others in the corpus.\n",
    "\n",
    "We also visualized the results using color-coded highlighting, where each top TF-IDF word in a document was highlighted with a unique color and accompanied by a bar chart showing the relative importance of each term. This allowed us to quickly identify the most informative words in each document and observe patterns across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee18b411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:06:23.083002Z",
     "iopub.status.busy": "2025-05-08T10:06:23.082656Z",
     "iopub.status.idle": "2025-05-08T10:06:23.106128Z",
     "shell.execute_reply": "2025-05-08T10:06:23.105385Z",
     "shell.execute_reply.started": "2025-05-08T10:06:23.082964Z"
    },
    "id": "ee18b411",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert the list of tokenized dictionaries into a DataFrame\n",
    "train_df_tokenized = pd.DataFrame(train_df_tokenized, columns=['context', 'question', 'answer'])\n",
    "\n",
    "train_df_tokenized.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4925a8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:06:23.113878Z",
     "iopub.status.busy": "2025-05-08T10:06:23.113594Z",
     "iopub.status.idle": "2025-05-08T10:06:28.019066Z",
     "shell.execute_reply": "2025-05-08T10:06:28.018532Z",
     "shell.execute_reply.started": "2025-05-08T10:06:23.113861Z"
    },
    "id": "e4925a8a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert tokenized lists back into strings for TF-IDF processing because TfidfVectorizer expects text input as strings\n",
    "train_df_tokenized['context_str'] = train_df_tokenized['context'].apply(lambda tokens: ' '.join(tokens))\n",
    "train_df_tokenized['question_str'] = train_df_tokenized['question'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Option 1: Apply TF-IDF on the 'context' field to compute the term frequency-inverse document frequency for words in the 'context' field\n",
    "vectorizer_context = TfidfVectorizer()\n",
    "tfidf_context = vectorizer_context.fit_transform(train_df_tokenized['context_str'])\n",
    "\n",
    "# Option 2: Apply TF-IDF on the 'question' field to compute the term frequency-inverse document frequency for words in the 'question' field\n",
    "vectorizer_question = TfidfVectorizer()\n",
    "tfidf_question = vectorizer_question.fit_transform(train_df_tokenized['question_str'])\n",
    "\n",
    "# Option 3: Apply TF-IDF on the combined 'context' and 'question' fields\n",
    "train_df_tokenized['combined'] = train_df_tokenized['context_str'] + ' ' + train_df_tokenized['question_str']\n",
    "\n",
    "# This vectorizer will compute the term frequency-inverse document frequency for words in the combined field\n",
    "vectorizer_combined = TfidfVectorizer()\n",
    "tfidf_combined = vectorizer_combined.fit_transform(train_df_tokenized['combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9641030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:06:28.019929Z",
     "iopub.status.busy": "2025-05-08T10:06:28.019746Z",
     "iopub.status.idle": "2025-05-08T10:06:41.793832Z",
     "shell.execute_reply": "2025-05-08T10:06:41.793185Z",
     "shell.execute_reply.started": "2025-05-08T10:06:28.019916Z"
    },
    "id": "c9641030",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert the sparse matrix of TF-IDF values for 'context' into a pandas DataFrame\n",
    "\n",
    "tfidf_context_df = pd.DataFrame(tfidf_context.toarray(), columns=vectorizer_context.get_feature_names_out())\n",
    "tfidf_question_df = pd.DataFrame(tfidf_question.toarray(), columns=vectorizer_question.get_feature_names_out())\n",
    "tfidf_combined_df = pd.DataFrame(tfidf_combined.toarray(), columns=vectorizer_combined.get_feature_names_out())\n",
    "\n",
    "print(\"Shape of TF-IDF Context DataFrame:\", tfidf_context_df.shape)\n",
    "print(\"Shape of TF-IDF Question DataFrame:\", tfidf_question_df.shape)\n",
    "print(\"Shape of TF-IDF Combined DataFrame:\", tfidf_combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab83a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:06:41.795380Z",
     "iopub.status.busy": "2025-05-08T10:06:41.794700Z",
     "iopub.status.idle": "2025-05-08T10:06:49.234665Z",
     "shell.execute_reply": "2025-05-08T10:06:49.233987Z",
     "shell.execute_reply.started": "2025-05-08T10:06:41.795360Z"
    },
    "id": "93ab83a2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "word_scores = tfidf_context_df.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# Plot top 20 words by TF-IDF score\n",
    "top_n = 20\n",
    "top_words = word_scores.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = plt.cm.tab20.colors  # Use a colormap for different colors\n",
    "top_words.plot(kind='bar', color=[colors[i % len(colors)] for i in range(len(top_words))])\n",
    "plt.title(f\"Top {top_n} Words by TF-IDF Score\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Total TF-IDF Score\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670a920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:06:49.236173Z",
     "iopub.status.busy": "2025-05-08T10:06:49.235672Z",
     "iopub.status.idle": "2025-05-08T10:08:09.062484Z",
     "shell.execute_reply": "2025-05-08T10:08:09.061863Z",
     "shell.execute_reply.started": "2025-05-08T10:06:49.236153Z"
    },
    "id": "6670a920",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Add a column to the DataFrame with the top N words for each document\n",
    "feature_names = vectorizer_context.get_feature_names_out()\n",
    "\n",
    "# Function to get top N words for a single document\n",
    "def get_top_n_words(row, n):\n",
    "    row_data = row.toarray().flatten()\n",
    "    top_indices = row_data.argsort()[::-1][:n]\n",
    "\n",
    "    return [(feature_names[i], row_data[i]) for i in top_indices if row_data[i] > 0]\n",
    "\n",
    "\n",
    "# Apply the function to each row of the TF-IDF DataFrame\n",
    "N = 5 # Number of top words to extract\n",
    "#Next to the word there is also the tfidf score\n",
    "train_df_tokenized[f'top_{N}_tfidf_words_context'] = [\n",
    "    get_top_n_words(tfidf_context[i], N) for i in range(tfidf_context.shape[0])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fafac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:08:09.064002Z",
     "iopub.status.busy": "2025-05-08T10:08:09.063260Z",
     "iopub.status.idle": "2025-05-08T10:08:09.074499Z",
     "shell.execute_reply": "2025-05-08T10:08:09.073897Z",
     "shell.execute_reply.started": "2025-05-08T10:08:09.063974Z"
    },
    "id": "070fafac",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df_tokenized.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c643b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:08:09.075761Z",
     "iopub.status.busy": "2025-05-08T10:08:09.075184Z",
     "iopub.status.idle": "2025-05-08T10:08:09.512180Z",
     "shell.execute_reply": "2025-05-08T10:08:09.511595Z",
     "shell.execute_reply.started": "2025-05-08T10:08:09.075739Z"
    },
    "id": "509c643b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import to_hex\n",
    "import random\n",
    "import re\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def highlight_words_in_text_colored(text, words_to_highlight, colors):\n",
    "    \"\"\"\n",
    "    Highlight specific words in a given text with corresponding colors.\n",
    "    \"\"\"\n",
    "    for (word, _), color in zip(words_to_highlight, colors):\n",
    "        # Use regex to match whole words only\n",
    "        text = re.sub(rf'\\b{re.escape(word)}\\b', f\"<mark style='background-color:{color};'>{word}</mark>\", text)\n",
    "    return text\n",
    "\n",
    "def plot_and_highlight_document_colored(doc_index, title='Top TF-IDF Words'):\n",
    "    \"\"\"\n",
    "    Plot top TF-IDF words with different colors and highlight them in the document with the same colors.\n",
    "    \"\"\"\n",
    "    word_score_pairs = train_df_tokenized.loc[doc_index, f'top_{N}_tfidf_words_context']\n",
    "    words, scores = zip(*word_score_pairs)\n",
    "\n",
    "    # Generate unique colors for each word\n",
    "    colors = [to_hex(plt.cm.tab10(i % 10)) for i in range(len(words))]\n",
    "\n",
    "    # Plot top TF-IDF words with different colors\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(words[::-1], scores[::-1], color=colors[::-1])  # reverse for descending order\n",
    "    plt.xlabel('TF-IDF Score')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Highlight words in the document with the same colors\n",
    "    highlighted_text = highlight_words_in_text_colored(train_df_tokenized.loc[doc_index, 'context_str'], word_score_pairs, colors)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.axis('off')\n",
    "    display(HTML(f\"<div style='font-size:14px; line-height:1.6;'>{highlighted_text}</div>\"))\n",
    "    plt.show()\n",
    "\n",
    "# Example: visualize the first 5 documents\n",
    "\n",
    "documents_to_show = 3\n",
    "for i in range(3):\n",
    "    plot_and_highlight_document_colored(i, title=f'Document {i+1} - Top TF-IDF Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee620a",
   "metadata": {},
   "source": [
    "# üóÇÔ∏è Dataset indexing & searching\n",
    "\n",
    "This section implements an efficient document retrieval system using TF-IDF (Term Frequency-Inverse Document Frequency) vectorization and cosine similarity. Here's what's happening:\n",
    "\n",
    "1. **Indexing**: We create a search index from our dataset's context fields by:\n",
    "   - Tokenizing each document into individual words\n",
    "   - Building a Gensim Dictionary that maps words to unique IDs\n",
    "   - Converting documents to bag-of-words representation\n",
    "   - Applying TF-IDF transformation to emphasize important terms\n",
    "   - Creating a similarity matrix for efficient retrieval\n",
    "\n",
    "2. **Search functionality**: We implement a search function that:\n",
    "   - Takes a user query and preprocesses it (tokenization, stopword removal)\n",
    "   - Converts the query to the same TF-IDF space as our corpus\n",
    "   - Calculates cosine similarity between the query and all documents\n",
    "   - Returns the top 3 most relevant documents with their similarity scores\n",
    "\n",
    "3. **Interactive interface**: We provide an interactive search widget where you can type any query in the search bar and see real-time results showing matching contexts, questions, answers, and relevance scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42867df7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:08:09.513148Z",
     "iopub.status.busy": "2025-05-08T10:08:09.512904Z",
     "iopub.status.idle": "2025-05-08T10:08:34.025604Z",
     "shell.execute_reply": "2025-05-08T10:08:34.024806Z",
     "shell.execute_reply.started": "2025-05-08T10:08:09.513124Z"
    },
    "id": "42867df7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Create the dictionary and corpus for the TF-IDF model\n",
    "texts = train_df['context']\n",
    "texts = [word_tokenize(text.lower()) for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#¬†Build the TF-IDF vectorizer\n",
    "tfidf_model = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "\n",
    "# Create the similarity index\n",
    "index = similarities.MatrixSimilarity(corpus_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d1cff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:08:34.026737Z",
     "iopub.status.busy": "2025-05-08T10:08:34.026463Z",
     "iopub.status.idle": "2025-05-08T10:08:34.039665Z",
     "shell.execute_reply": "2025-05-08T10:08:34.038938Z",
     "shell.execute_reply.started": "2025-05-08T10:08:34.026706Z"
    },
    "id": "815d1cff",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Function to get the most similar documents\n",
    "def search(query_tokens, top_k=3):\n",
    "    query_bow = dictionary.doc2bow(query_tokens)\n",
    "    query_tfidf = tfidf_model[query_bow]\n",
    "    sims = index[query_tfidf]  # cosine similarities\n",
    "    top_indices = sorted(enumerate(sims), key=lambda x: -x[1])[:top_k]\n",
    "    results = train_df.iloc[[i for i, _ in top_indices]].copy()\n",
    "    results['similarity'] = [score for _, score in top_indices]\n",
    "    return results[['context', 'question', 'answer', 'similarity']]\n",
    "\n",
    "# Create an output widget for displaying the results\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Function to handle search queries\n",
    "def interactive_search(change):\n",
    "    query = change['new']\n",
    "    query_tokens = preprocess(query)  # Tokenize and preprocess the query\n",
    "    result_df = search(query_tokens)\n",
    "    with output_area:\n",
    "        clear_output(wait=True)  # Clear only the output area\n",
    "        output_area.append_display_data(result_df)  # Append the resulting dataframe to the output area\n",
    "\n",
    "# Create a text input widget\n",
    "search_bar = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter your search query',\n",
    "    description='Search:',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "\n",
    "# Add an observer to the search bar\n",
    "search_bar.observe(interactive_search, names='value')\n",
    "\n",
    "# Display the search bar and output area\n",
    "display(search_bar, output_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050162dd",
   "metadata": {
    "id": "050162dd"
   },
   "source": [
    "# üìù Sentence embedding (all-MiniLM-L6-v2 on context)\n",
    "\n",
    "This section converts text contexts into dense vector representations using the `all-MiniLM-L6-v2` model from SentenceTransformer. Unlike word-level embeddings, sentence embeddings capture semantic meaning for entire text passages in a fixed-size vector. The model used maps sentences & paragraphs to a 384-dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "\n",
    "The embedding is performed on the context feature of the dataset, and we then use the function `community_detection` to find, in the embeddings, all communities (i.e., embeddings that are close‚Äîcloser than a specified threshold). This is used for clustering. We then plot the distribution of the clusters and decide to use BERTopic to effectively cluster the context feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9f75a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:08:34.041312Z",
     "iopub.status.busy": "2025-05-08T10:08:34.040422Z",
     "iopub.status.idle": "2025-05-08T10:09:24.015894Z",
     "shell.execute_reply": "2025-05-08T10:09:24.015359Z",
     "shell.execute_reply.started": "2025-05-08T10:08:34.041289Z"
    },
    "id": "70c9f75a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Model for computing sentence embeddings.\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentence_embeddings = model.encode(train_df['context'].tolist(), convert_to_tensor=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d7d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:09:24.016996Z",
     "iopub.status.busy": "2025-05-08T10:09:24.016716Z",
     "iopub.status.idle": "2025-05-08T10:09:24.543571Z",
     "shell.execute_reply": "2025-05-08T10:09:24.542979Z",
     "shell.execute_reply.started": "2025-05-08T10:09:24.016976Z"
    },
    "id": "5f5d7d74",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Perform community detection on the sentence embeddings\n",
    "clusters = util.community_detection(sentence_embeddings, min_community_size=10, threshold=0.6, show_progress_bar=True)\n",
    "\n",
    "print(\"Number of clusters found:\", len(clusters))\n",
    "cluster_sizes = [len(cluster) for cluster in clusters]\n",
    "print(\"Cluster coverage:\", sum(cluster_sizes) / len(train_df))\n",
    "\n",
    "# Plot the distribution of cluster sizes\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(cluster_sizes, kde=True)\n",
    "plt.title('Distribution of Cluster Sizes')\n",
    "plt.xlabel('Cluster Size')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b53e63",
   "metadata": {
    "id": "65b53e63"
   },
   "source": [
    "# üé© Topic Modelling (BERTopic)\n",
    "\n",
    "BERTopic is an advanced topic modeling library that leverages transformer embeddings to discover topics within large text collections. Here's how it works:\n",
    "\n",
    "**Core process:**\n",
    "- Uses SentenceTransformer embeddings as semantic document representations\n",
    "- Applies HDBSCAN clustering to group similar documents\n",
    "- Creates topic representations by extracting distinctive keywords using c-TF-IDF\n",
    "- Generates hierarchical topic structures and visualizations\n",
    "\n",
    "Furthermore, BERTopic provides a set of built-in visualization methods to explore the topics generated.\n",
    "\n",
    "**Why use BERTopic instead of SentenceTransformer alone?**\n",
    "\n",
    "While SentenceTransformer (e.g., all-MiniLM-L6-v2) provides powerful sentence embeddings for converting text into dense vectors, it does not offer an end-to-end topic modeling pipeline. BERTopic builds on top of these embeddings and adds robust clustering (using HDBSCAN), topic representation, and visualization capabilities. This makes BERTopic especially effective for discovering interpretable topics in large text datasets, whereas SentenceTransformer alone only provides representations without direct topic extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d77ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install bertopic if not already installed\n",
    "\n",
    "if IN_KAGGLE or IN_COLAB:\n",
    "  ! pip install -q bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c79fc4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:17:06.178903Z",
     "iopub.status.busy": "2025-05-08T10:17:06.178507Z",
     "iopub.status.idle": "2025-05-08T10:18:06.420610Z",
     "shell.execute_reply": "2025-05-08T10:18:06.419706Z",
     "shell.execute_reply.started": "2025-05-08T10:17:06.178878Z"
    },
    "id": "8c79fc4e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Create a BERTopic model\n",
    "topic_model = BERTopic(verbose=True)\n",
    "topics, probs = topic_model.fit_transform(train_df['context'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f8446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the topics for each document to the DataFrame\n",
    "train_df['topic'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e78c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_records = len(train_df)\n",
    "records_with_topic = len(train_df[train_df['topic'] != -1])\n",
    "percentage_with_topic = (records_with_topic / total_records) * 100\n",
    "\n",
    "print(f\"Number of records with any topic assigned: {records_with_topic} ({percentage_with_topic:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a5281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_relevant_words = {}\n",
    "for topic in set(topics):\n",
    "    topic_relevant_words[topic] = [word for word, _ in topic_model.get_topic(topic)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7616fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of unique topics:\", len(topic_relevant_words))\n",
    "topic_relevant_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4718753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:18:20.171419Z",
     "iopub.status.busy": "2025-05-08T10:18:20.171121Z",
     "iopub.status.idle": "2025-05-08T10:18:20.376759Z",
     "shell.execute_reply": "2025-05-08T10:18:20.376066Z",
     "shell.execute_reply.started": "2025-05-08T10:18:20.171403Z"
    },
    "id": "b4718753",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#¬†Visualize the topics\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b36d68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:18:20.377770Z",
     "iopub.status.busy": "2025-05-08T10:18:20.377563Z",
     "iopub.status.idle": "2025-05-08T10:19:07.853397Z",
     "shell.execute_reply": "2025-05-08T10:19:07.852714Z",
     "shell.execute_reply.started": "2025-05-08T10:18:20.377754Z"
    },
    "id": "77b36d68",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# visualize the document representation\n",
    "topic_model.visualize_documents(train_df['context'].tolist(), topics = list(range(20)), hide_document_hover=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ddc71e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:19:07.854552Z",
     "iopub.status.busy": "2025-05-08T10:19:07.854274Z",
     "iopub.status.idle": "2025-05-08T10:19:07.859138Z",
     "shell.execute_reply": "2025-05-08T10:19:07.858198Z",
     "shell.execute_reply.started": "2025-05-08T10:19:07.854528Z"
    },
    "id": "28ddc71e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_document_datamap(train_df['context'].tolist(), enable_search=True, interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c06cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:19:07.861383Z",
     "iopub.status.busy": "2025-05-08T10:19:07.861095Z",
     "iopub.status.idle": "2025-05-08T10:19:08.044845Z",
     "shell.execute_reply": "2025-05-08T10:19:08.044114Z",
     "shell.execute_reply.started": "2025-05-08T10:19:07.861362Z"
    },
    "id": "5a9c06cd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# visualize the topic representation\n",
    "topic_model.visualize_barchart(top_n_topics=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815a68c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:19:08.045778Z",
     "iopub.status.busy": "2025-05-08T10:19:08.045573Z",
     "iopub.status.idle": "2025-05-08T10:19:08.107664Z",
     "shell.execute_reply": "2025-05-08T10:19:08.106336Z",
     "shell.execute_reply.started": "2025-05-08T10:19:08.045763Z"
    },
    "id": "3815a68c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "topic_no = 27\n",
    "\n",
    "print(f\"Topic {topic_no}:\")\n",
    "# get the topic representation\n",
    "for topic, prob in topic_model.get_topic(topic_no):\n",
    "    print(f\"{topic}: {prob:.2f}\")\n",
    "# find all the documents in a topic and relative topics in a pretty format\n",
    "for i in topic_model.get_representative_docs(topic_no):\n",
    "\n",
    "    index = train_df[train_df['context'].str.contains(i)].index[0]\n",
    "    print(f'Document {index}\\nContext: {i}')\n",
    "    # find the record in the original dataset by a serach on the context and print question and answer\n",
    "    print(\"Question:\", train_df[train_df['context'].str.contains(i)]['question'].values[0])\n",
    "    print(\"Answer:\", train_df[train_df['context'].str.contains(i)]['answer'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184b74ea",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-08T10:19:08.108118Z",
     "iopub.status.idle": "2025-05-08T10:19:08.108366Z",
     "shell.execute_reply": "2025-05-08T10:19:08.108269Z",
     "shell.execute_reply.started": "2025-05-08T10:19:08.108257Z"
    },
    "id": "184b74ea",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# sviualize the word topics without the document representation\n",
    "topic_model.visualize_heatmap(top_n_topics=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e61c1d",
   "metadata": {},
   "source": [
    "## Hierarchical clustering on topics\n",
    "\n",
    "BERTopic enables hierarchical organization of topics using agglomerative clustering, which progressively merges similar topics into larger clusters. This creates a tree-like structure showing topic relationships at different granularity levels.\n",
    "\n",
    "However, as clusters grow larger in the hierarchy, clustering quality tends to deteriorate significantly. This is because semantic coherence decreases when diverse topics are merged, leading to a loss of topic specificity and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5b5af",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-08T10:19:08.109920Z",
     "iopub.status.idle": "2025-05-08T10:19:08.110202Z",
     "shell.execute_reply": "2025-05-08T10:19:08.110067Z",
     "shell.execute_reply.started": "2025-05-08T10:19:08.110051Z"
    },
    "id": "87a5b5af",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(train_df['context'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19007a44",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-08T10:19:08.111263Z",
     "iopub.status.idle": "2025-05-08T10:19:08.111539Z",
     "shell.execute_reply": "2025-05-08T10:19:08.111404Z",
     "shell.execute_reply.started": "2025-05-08T10:19:08.111389Z"
    },
    "id": "19007a44",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad74826",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-08T10:19:08.112455Z",
     "iopub.status.idle": "2025-05-08T10:19:08.112651Z",
     "shell.execute_reply": "2025-05-08T10:19:08.112566Z",
     "shell.execute_reply.started": "2025-05-08T10:19:08.112557Z"
    },
    "id": "fad74826",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "itables.show(hierarchical_topics.sort_values(by='Distance', ascending=False), max_rows=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c620fe",
   "metadata": {},
   "source": [
    "# üíä Discover Context Containing Toxic Content\n",
    "\n",
    "This section identifies and labels toxic content using a weakly supervised approach:\n",
    "\n",
    "1. **Topic Extraction**: Topics and keywords are extracted using BERTopic.\n",
    "2. **Toxic Keyword Detection**: Profanity words are downloaded and matched against topic keywords to identify toxic topics.\n",
    "3. **Filtering**: Non-toxic false positives (e.g., \"wang\", \"sexy\") are removed, and topics containing fewer than 4 toxic-relevant words are excluded to reduce noise.\n",
    "4. **Labeling**: Documents linked to toxic topics are labeled as toxic (`1`).\n",
    "\n",
    "This method efficiently labels toxic content without manual annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0742d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to list\n",
    "topics_list = []\n",
    "for topic, words in topic_model.get_topics().items():\n",
    "    for word in words:\n",
    "        topics_list.append(word[0])\n",
    "\n",
    "# count the frequency of the relevant words\n",
    "topic_word_counts = Counter(topics_list)\n",
    "\n",
    "# remove duplicates\n",
    "topics_list = list(set(topics_list))\n",
    "len(topics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=800, height=500).generate_from_frequencies(topic_word_counts)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud of Topics\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34132d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 20 most frequent words\n",
    "n = 30\n",
    "top_n_words = dict(sorted(topic_word_counts.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "\n",
    "# Plot the frequency of the top 20 relevant words as a horizontal bar chart\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(list(top_n_words.keys()), list(top_n_words.values()), color='skyblue')\n",
    "plt.title(f'Frequency of Top {n} Relevant Words in Topics')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Relevant Words')\n",
    "plt.gca().invert_yaxis()  # To display the most frequent word at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa34be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†download profanity words\n",
    "!wget https://raw.githubusercontent.com/coffee-and-fun/google-profanity-words/refs/heads/main/data/en.txt -O profanity_words.txt\n",
    "\n",
    "# Read the profanity words into a set\n",
    "with open('profanity_words.txt', 'r') as f:\n",
    "    profanity_words = set(f.read().splitlines())\n",
    "\n",
    "toxic_topics = []\n",
    "\n",
    "for topic in topics_list:\n",
    "    if topic in profanity_words:\n",
    "        toxic_topics.append(topic)\n",
    "\n",
    "print(\"Toxic topics:\", toxic_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65dc6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"wang\" in toxic_topics:\n",
    "    toxic_topics.remove(\"wang\")\n",
    "if \"sexy\" in toxic_topics:\n",
    "    toxic_topics.remove(\"sexy\")\n",
    "\n",
    "toxic_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a72c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_topic_ids = []\n",
    "\n",
    "for id, words in topic_relevant_words.items():\n",
    "    for word in words:\n",
    "        if word in toxic_topics:\n",
    "            toxic_topic_ids.append(id)\n",
    "            break\n",
    "\n",
    "toxic_topic_ids = list(set(toxic_topic_ids))\n",
    "toxic_topic_ids\n",
    "\n",
    "for id in toxic_topic_ids:\n",
    "    print(f\"Topic {id}: {topic_relevant_words[id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c363f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of relevant words that are toxic for each topic using the profanity_words\n",
    "toxic_word_counts = {}\n",
    "for topic, words in topic_relevant_words.items():\n",
    "    toxic_word_counts[topic] = len([word for word in words if word in toxic_topics])\n",
    "\n",
    "# remove topics with less than 5 toxic words\n",
    "for id in toxic_topic_ids:\n",
    "    if toxic_word_counts[id] < 4:\n",
    "        toxic_topic_ids.remove(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the toxic topics after filtering\n",
    "print(\"Filtered Toxic Topics:\")\n",
    "for id in toxic_topic_ids:\n",
    "    print(f\"Topic {id}: {topic_relevant_words[id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea22559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_toxic_topics(row):\n",
    "    if row['topic'] in toxic_topic_ids:\n",
    "        return int(1)\n",
    "    else:\n",
    "        return int(0)\n",
    "    \n",
    "# # Apply the function to the DataFrame\n",
    "tqdm.pandas(desc=\"Labeling toxic topics\")\n",
    "train_df['toxic'] = train_df.progress_apply(label_toxic_topics, axis=1)\n",
    "\n",
    "# Count the number of toxic topics\n",
    "toxic_count = train_df['toxic'].sum()\n",
    "print(f\"Number of toxic topics: {int(toxic_count)}\")\n",
    "print(f\"Percentage of toxic topics: {toxic_count / len(train_df) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda50457",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df['toxic'] == 1].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323a79d",
   "metadata": {},
   "source": [
    "## Train binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec8df84",
   "metadata": {},
   "source": [
    "# üíä Train Binary Classifier\n",
    "\n",
    "After identifying toxic records, this section implements a BERT-based binary classifier to automatically detect toxic content in new texts:\n",
    "\n",
    "**Data preparation:**\n",
    "- Created a balanced dataset by undersampling non-toxic examples (3:1 ratio)\n",
    "- Removed records lacking topic assignments\n",
    "- Split data into train (80%), validation (8%) and test (12%) sets with stratification\n",
    "\n",
    "The model uses the `bert-base-multilingual-uncased` architecture, fine-tuned for sequence classification tasks. Its performance was evaluated using a confusion matrix alongside standard classification metrics.\n",
    "\n",
    "The classifier can determine if new text contains toxic content with a simple function call, complementing the topic-based detection approach with direct predictive capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0cf5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_binary = train_df[['context', 'question', 'answer', 'toxic', 'topic']].copy()\n",
    "# remove records with cluster -1\n",
    "train_df_binary = train_df_binary[train_df_binary['topic'] != -1]\n",
    "print(\"Number of records with cluster -1:\", len(train_df) - len(train_df_binary))\n",
    "train_df_binary = train_df_binary.reset_index(drop=True)\n",
    "train_df_binary.drop(columns=['topic'], inplace=True)\n",
    "train_df_binary.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of toxic and non-toxic records\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='toxic', data=train_df_binary)\n",
    "plt.title('Distribution of Toxic and Non-Toxic Records')\n",
    "plt.xlabel('Toxicity')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Non-Toxic', 'Toxic'])\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of toxic records:\", len(train_df_binary[train_df_binary['toxic'] == 1]))\n",
    "print(\"Number of non-toxic records:\", len(train_df_binary[train_df_binary['toxic'] == 0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bffb43a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> BERTopic clustered approximately 50% of the records. The remaining records were not assigned to any topic, likely due to insufficient semantic similarity to form coherent clusters.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4004df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = train_df_binary[train_df_binary['toxic'] == 0]\n",
    "minority_class = train_df_binary[train_df_binary['toxic'] == 1]\n",
    "\n",
    "# Perform undersampling on the majority class to match the size of the minority class\n",
    "majority_class_undersampled = resample(\n",
    "    majority_class,\n",
    "    replace=False,  # Sample without replacement\n",
    "    n_samples=len(minority_class)*3,  # Match the minority class size\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Combine the undersampled majority class with the minority class\n",
    "balanced_df = pd.concat([majority_class_undersampled, minority_class])\n",
    "\n",
    "# Split the balanced dataset into training and testing sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
    "    balanced_df['context'],\n",
    "    balanced_df['toxic'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=balanced_df['toxic']\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val_test,\n",
    "    y_val_test,\n",
    "    test_size=0.4,\n",
    "    random_state=42,\n",
    "    stratify=y_val_test\n",
    ")\n",
    "\n",
    "print(\"Train set size:\", len(X_train))\n",
    "print(\"Validation set size:\", len(X_val))\n",
    "print(\"Test set size:\", len(X_test))\n",
    "print(\"Class distribution in train set:\\n\", y_train.value_counts())\n",
    "print(\"Class distribution in test set:\\n\", y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of toxic and non-toxic records\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='toxic', data=balanced_df)\n",
    "plt.title('Distribution of Toxic and Non-Toxic Records')\n",
    "plt.xlabel('Toxicity')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Non-Toxic', 'Toxic'])\n",
    "plt.show()\n",
    "\n",
    "print(\"Number of toxic records:\", len(balanced_df[balanced_df['toxic'] == 1]))\n",
    "print(\"Number of non-toxic records:\", len(balanced_df[balanced_df['toxic'] == 0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7784224",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-multilingual-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d992661",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(y_train.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"Using GPU\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Using Apple Silicon GPU\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f86fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910e5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce08e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d698cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [{'text': txt, 'label': lbl} for txt, lbl in zip(X_train, y_train)]\n",
    "val_data = [{'text': txt, 'label': lbl} for txt, lbl in zip(X_val, y_val)]\n",
    "test_data = [{'text': txt, 'label': lbl} for txt, lbl in zip(X_test, y_test)]\n",
    "\n",
    "train_data = Dataset.from_list(train_data)\n",
    "test_data = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed7af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['validation'] = test_data\n",
    "data['test'] = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True, desc=\"Tokenizing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f041e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid the train on local machine\n",
    "if IN_COLAB or IN_KAGGLE:\n",
    "    BINARY_CLASSIFIER_TRAINING = True\n",
    "else:\n",
    "    BINARY_CLASSIFIER_TRAINING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f382e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BINARY_CLASSIFIER_TRAINING:\n",
    "    ! pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BINARY_CLASSIFIER_TRAINING:\n",
    "    os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        \"toxic-bert\",\n",
    "        per_device_train_batch_size=16,\n",
    "        report_to=None,  # Disabling wandb callbacks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18beb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BINARY_CLASSIFIER_TRAINING:\n",
    "    trainer = Trainer(\n",
    "        model=bert,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_data['train'],\n",
    "        eval_dataset=tokenized_data['validation']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b6b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BINARY_CLASSIFIER_TRAINING:\n",
    "    # Train the model\n",
    "    trainer.train() \n",
    "\n",
    "    # Save the model\n",
    "    trainer.save_model(\"toxic-bert\")\n",
    "    tokenizer.save_pretrained(\"toxic-bert\")\n",
    "    print(\"Model saved to toxic-bert directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ef595",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BINARY_CLASSIFIER_TRAINING:\n",
    "    # Train the model\n",
    "    trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b682fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90992a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWLOAD_MODEL_KAGGLE = False\n",
    "\n",
    "if DOWLOAD_MODEL_KAGGLE:\n",
    "    \n",
    "    import kagglehub\n",
    "    kagglehub.model_download('martinimarcello00/bert/pytorch/bert-toxic-classifier')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a8bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_path = \"./toxic-bert\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Move to appropriate device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BINARY_CLASSIFIER_TRAINING:\n",
    "    preds = trainer.predict(tokenized_data['test'])\n",
    "    y_pred = torch.argmax(torch.tensor(preds.predictions), dim=1).numpy()\n",
    "else:\n",
    "        # Get the test data\n",
    "    test_texts = X_test.tolist()\n",
    "    test_encodings = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Move tensors to the appropriate device\n",
    "    device = model.device\n",
    "    input_ids = test_encodings['input_ids'].to(device)\n",
    "    attention_mask = test_encodings['attention_mask'].to(device)\n",
    "\n",
    "    # Get predictions from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = outputs.logits\n",
    "\n",
    "    # Convert to numpy array\n",
    "    preds = predictions.cpu().numpy()\n",
    "\n",
    "    # Get predicted class (0 or 1)\n",
    "    y_pred = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7e06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['Non-Toxic', 'Toxic']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a12ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Toxic', 'Toxic'], yticklabels=['Non-Toxic', 'Toxic'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c990efe",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b> The model is likely overfitting, as we performed undersampling on the toxic class, which represented less than 1% of the dataset.\n",
    "We chose to proceed since the pipeline functions correctly, but additional data are needed to effectively fine-tune BERT for binary toxic classification.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e89e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_toxicity(text):\n",
    "    # Prepare text for model\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "        \n",
    "    return \"Toxic\" if prediction == 1 else \"Non-Toxic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7624636",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predict_toxicity(\"Hello we are studyng NLP together\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da455209",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = predict_toxicity(\"I like to lick pussy and use my cock to ride every fucking girl\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f24240",
   "metadata": {
    "id": "76f24240"
   },
   "source": [
    "# üîÆ Google Gemma 2b & Google T5 LLM model QA ability testing\n",
    "\n",
    "In this section, we evaluated the question-answering capabilities of two language models:\n",
    "\n",
    "- Models Used: Google's **gemma-2b-it** and Google's **flan-t5-base**.\n",
    "- Testing Approach: We tested both models in two scenarios:\n",
    "    - Zero-shot: Models were given only the question with no additional context.\n",
    "    - Context-enhanced: Models were provided with relevant context alongside the question.\n",
    "\n",
    "## Zero-Shot vs. Context-Enhanced Testing\n",
    "\n",
    "**Zero-shot** testing evaluates a model's ability to answer questions using only its pre-trained knowledge. This approach tests what the model \"knows\" implicitly from its training data, without any additional information. We use zero-shot testing to assess the model's baseline capabilities and inherent knowledge limitations.\n",
    "\n",
    "**Context-enhanced** testing provides the model with relevant information alongside the question. This simulates real-world retrieval-augmented generation (RAG) systems, where models can access external knowledge sources to improve their answers. By providing context, we test the model's ability to extract and synthesize information from provided text rather than relying solely on its parametric knowledge.\n",
    "\n",
    "As expected, both models showed significantly improved performance when provided with relevant context. This confirms that while these language models contain impressive amounts of knowledge, they still benefit substantially from access to specific information relevant to the question at hand. The context-enhanced approach helps overcome limitations in the models' internal knowledge and reduces hallucinations by grounding responses in factual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required for T5 model\n",
    "! pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b9ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:19:35.641634Z",
     "iopub.status.busy": "2025-05-08T10:19:35.640942Z",
     "iopub.status.idle": "2025-05-08T10:19:35.645844Z",
     "shell.execute_reply": "2025-05-08T10:19:35.645125Z",
     "shell.execute_reply.started": "2025-05-08T10:19:35.641611Z"
    },
    "id": "0e8b9ff7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435829a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:19:38.426152Z",
     "iopub.status.busy": "2025-05-08T10:19:38.425873Z",
     "iopub.status.idle": "2025-05-08T10:20:03.576814Z",
     "shell.execute_reply": "2025-05-08T10:20:03.576153Z",
     "shell.execute_reply.started": "2025-05-08T10:19:38.426135Z"
    },
    "id": "9435829a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if IN_KAGGLE or IN_COLAB:\n",
    "    torch.set_default_device(\"cuda\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", trust_remote_code=True)\n",
    "\n",
    "gemma_tokenizer.pad_token = gemma_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b247fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "print(t5_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SsksfJE3z_es",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:20:14.475486Z",
     "iopub.status.busy": "2025-05-08T10:20:14.474981Z",
     "iopub.status.idle": "2025-05-08T10:20:17.631919Z",
     "shell.execute_reply": "2025-05-08T10:20:17.631271Z",
     "shell.execute_reply.started": "2025-05-08T10:20:14.475452Z"
    },
    "id": "SsksfJE3z_es",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not IN_KAGGLE and not IN_COLAB:\n",
    "    # Move the model to GPU if available\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    gemma_model.to(device)\n",
    "    t5_model.to(device)\n",
    "else:\n",
    "    gemma_model.to(\"cuda\")\n",
    "    t5_model.to(\"cuda\")\n",
    "\n",
    "\n",
    "print(\"Gemma model loaded successfully.\")\n",
    "print(\"T5 model loaded successfully.\")\n",
    "print(\"Gemma model device:\", gemma_model.device)\n",
    "print(\"T5 model device:\", t5_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11273c03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:20:21.953947Z",
     "iopub.status.busy": "2025-05-08T10:20:21.953712Z",
     "iopub.status.idle": "2025-05-08T10:20:22.158170Z",
     "shell.execute_reply": "2025-05-08T10:20:22.157407Z",
     "shell.execute_reply.started": "2025-05-08T10:20:21.953929Z"
    },
    "id": "11273c03",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_test = 50\n",
    "\n",
    "train_df = dataset['train'].to_pandas()\n",
    "train_df = train_df.dropna()\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "subset = train_df.sample(n_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c72464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:20:24.234837Z",
     "iopub.status.busy": "2025-05-08T10:20:24.234136Z",
     "iopub.status.idle": "2025-05-08T10:20:24.238783Z",
     "shell.execute_reply": "2025-05-08T10:20:24.237976Z",
     "shell.execute_reply.started": "2025-05-08T10:20:24.234812Z"
    },
    "id": "f7c72464",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def store_answer(entry, generation_function, context=False):\n",
    "    question = entry['question']\n",
    "    context = entry['context']\n",
    "    if context:\n",
    "        answer = generation_function(question, context)\n",
    "    else:\n",
    "        answer = generation_function(question)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4624ae1",
   "metadata": {
    "id": "a4624ae1"
   },
   "source": [
    "## Zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d77a9f4-43a8-46c4-811b-f64a7d61e0d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:57:37.469350Z",
     "iopub.status.busy": "2025-05-08T12:57:37.468759Z",
     "iopub.status.idle": "2025-05-08T12:57:37.474812Z",
     "shell.execute_reply": "2025-05-08T12:57:37.474271Z",
     "shell.execute_reply.started": "2025-05-08T12:57:37.469328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Initialize variables\n",
    "gemma_df = pd.DataFrame()\n",
    "t5_df = pd.DataFrame()\n",
    "\n",
    "# Initialize booleans to track file existence\n",
    "gemma_exists = False\n",
    "t5_exists = False\n",
    "\n",
    "try:\n",
    "    # Check and load subset_with_generated_answers_with_context_gemma.csv\n",
    "    if os.path.exists('subset_with_generated_answers_with_context_gemma.csv'):\n",
    "        gemma_df = pd.read_csv('subset_with_generated_answers_with_context_gemma.csv')\n",
    "        gemma_exists = True\n",
    "    else:\n",
    "        gemma_df['question'] = subset['question']\n",
    "        gemma_df['context'] = subset['context']\n",
    "        gemma_df['answer'] = subset['answer']\n",
    "\n",
    "    # Check and load subset_with_generated_answers_with_context_t5.csv\n",
    "    if os.path.exists('subset_with_generated_answers_with_context_t5.csv'):\n",
    "        t5_df = pd.read_csv('subset_with_generated_answers_with_context_t5.csv')\n",
    "        t5_exists = True\n",
    "    else:\n",
    "        t5_df['question'] = subset['question']\n",
    "        t5_df['context'] = subset['context']\n",
    "        t5_df['answer'] = subset['answer']\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Print the status of file loading\n",
    "print(f\"subset_with_generated_answers_with_context_gemma.csv loaded: {gemma_exists}\")\n",
    "print(f\"subset_with_generated_answers_with_context_t5.csv loaded: {t5_exists}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058a751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:57:40.091470Z",
     "iopub.status.busy": "2025-05-08T12:57:40.090881Z",
     "iopub.status.idle": "2025-05-08T12:57:40.095790Z",
     "shell.execute_reply": "2025-05-08T12:57:40.095148Z",
     "shell.execute_reply.started": "2025-05-08T12:57:40.091447Z"
    },
    "id": "f058a751",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_answer_zeroshot(entry,model,tokenizer):\n",
    "    question = entry['question']\n",
    "    if model == gemma_model:\n",
    "\n",
    "        prompt = f\"\"\"You are a precise and concise assistant.\n",
    "    Answer the question briefly and do not add anything else‚Äîno explanations, comments, or formatting.\n",
    "\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "        \n",
    "    else:\n",
    "      prompt = f\"Answer the following question: {question}\"\n",
    "    \n",
    "    if not IN_KAGGLE and not IN_COLAB:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to('mps')  # Use \"cuda\" or correct device\n",
    "    else:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    if model == gemma_model:\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    else:  \n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    if(model == gemma_model):\n",
    "            # Remove everything before the first occurrence of \"Answer:\" (if needed)\n",
    "        match = re.search(r'Answer:\\s*(.*)', text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        else:\n",
    "            return text.strip()\n",
    "    else:\n",
    "        # Clean the T5 output\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec544f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T10:20:52.825999Z",
     "iopub.status.busy": "2025-05-08T10:20:52.825736Z",
     "iopub.status.idle": "2025-05-08T10:25:43.016150Z",
     "shell.execute_reply": "2025-05-08T10:25:43.015584Z",
     "shell.execute_reply.started": "2025-05-08T10:20:52.825979Z"
    },
    "id": "dec544f5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not gemma_exists:\n",
    "    tqdm.pandas(desc=\"Generating answers using zero-shot gemma-2b-it\")\n",
    "    gemma_df['zeroShot_answer'] = subset.progress_apply(lambda row: generate_answer_zeroshot(row, gemma_model, gemma_tokenizer), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3939218",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not t5_exists:\n",
    "    tqdm.pandas(desc=\"Generating answers using zero-shot phi-2\")\n",
    "    t5_df['zeroShot_answer'] = subset.progress_apply(lambda row: generate_answer_zeroshot(row, t5_model, t5_tokenizer), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd300f0-b11a-4787-af90-1848e0c68690",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-08T10:26:14.692332Z",
     "iopub.status.busy": "2025-05-08T10:26:14.691748Z",
     "iopub.status.idle": "2025-05-08T10:26:14.703426Z",
     "shell.execute_reply": "2025-05-08T10:26:14.702693Z",
     "shell.execute_reply.started": "2025-05-08T10:26:14.692311Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gemma_df[['question', 'answer', 'zeroShot_answer']].head(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54526ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_df[['question', 'answer', 'zeroShot_answer']].head(n_test)\n",
    "\n",
    "t5_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d722ea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:57:47.475352Z",
     "iopub.status.busy": "2025-05-08T12:57:47.474674Z",
     "iopub.status.idle": "2025-05-08T12:57:47.479784Z",
     "shell.execute_reply": "2025-05-08T12:57:47.479046Z",
     "shell.execute_reply.started": "2025-05-08T12:57:47.475327Z"
    },
    "id": "6d722ea6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_answer_zeroshot_context(entry, model, tokenizer):\n",
    "    question = entry['question']\n",
    "    context = entry.get('context', '')  # safely get context if exists\n",
    "\n",
    "    if model == gemma_model:\n",
    "        prompt = f\"\"\"You are a precise and concise assistant.\n",
    "Answer the question briefly and do not add anything else‚Äîno explanations, comments, or formatting.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"You are a precise and concise assistant.\n",
    "        Use the context below to answer the question accurately.\n",
    "        Provide only the answer; do not repeat the question or the context.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "    # Set device\n",
    "    device = \"cuda\" if IN_KAGGLE or IN_COLAB else \"mps\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate output with different parameters per model\n",
    "    if model == gemma_model:\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    else:\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=100\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract and clean output depending on model\n",
    "    if model == gemma_model:\n",
    "        match = re.search(r'Answer:\\s*(.*)', text, re.DOTALL)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "        else:\n",
    "            return text.strip()\n",
    "    else:\n",
    "        return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f121d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T12:57:49.527572Z",
     "iopub.status.busy": "2025-05-08T12:57:49.527304Z",
     "iopub.status.idle": "2025-05-08T13:03:09.966616Z",
     "shell.execute_reply": "2025-05-08T13:03:09.965895Z",
     "shell.execute_reply.started": "2025-05-08T12:57:49.527552Z"
    },
    "id": "1a1f121d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not gemma_exists:\n",
    "    tqdm.pandas(desc=\"Generating answers using zero-shot gemma-2b-it with context\")\n",
    "    gemma_df['zeroShot_context_answer'] = subset.progress_apply(lambda row: generate_answer_zeroshot_context(row, gemma_model, gemma_tokenizer), axis=1)\n",
    "\n",
    "    gemma_df.to_csv('subset_with_generated_answers_with_context_gemma.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cE5uRu2Tma",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-08T13:04:14.957578Z",
     "iopub.status.busy": "2025-05-08T13:04:14.956983Z",
     "iopub.status.idle": "2025-05-08T13:04:14.965996Z",
     "shell.execute_reply": "2025-05-08T13:04:14.965372Z",
     "shell.execute_reply.started": "2025-05-08T13:04:14.957554Z"
    },
    "id": "e0cE5uRu2Tma",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "gemma_df[['question', 'answer','zeroShot_answer', 'zeroShot_context_answer']].head(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1403eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not t5_exists:\n",
    "    tqdm.pandas(desc=\"Generating answers using zero-shot T5 with context\")\n",
    "    t5_df['zeroShot_context_answer'] = subset.progress_apply(lambda row: generate_answer_zeroshot_context(row, t5_model, t5_tokenizer), axis=1)\n",
    "\n",
    "    t5_df.to_csv('subset_with_generated_answers_with_context_t5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca07a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_df[['question', 'answer','zeroShot_answer', 'zeroShot_context_answer']].head(n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d92bf",
   "metadata": {},
   "source": [
    "## üë©üèª‚Äçüè´ Gemma 2b and T5 evaluation\n",
    "\n",
    "In this section, we evaluate and compare the Gemma 2B and T5 models using the bert_score library. The evaluation includes several metrics like: Exact Match, F1 Score, BERTScore Precision, Recall, and F1. The models are assessed under two conditions: with and without context given.\n",
    "\n",
    "**Exact match** is a metric that checks whether the predicted answer matches the ground truth exactly, including punctuation and word order. It is a strict metric commonly used in question-answering tasks. Due to the variability in natural language generation, EM is often low even when predictions are semantically correct.\n",
    "\n",
    "**Prediction** is the fraction of predicted words that are correct while **Recall** is the fraction of reference words that are correctly predicted.\n",
    "\n",
    "$\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}$\n",
    "\n",
    "$\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}$\n",
    "\n",
    "\n",
    "**F1 score** is computed as an harmonic mean of precision and recall.\n",
    "\n",
    "$\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "\n",
    "F1 is more forgiving than EM and reflects partial correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e5a5e6",
   "metadata": {},
   "source": [
    "### BertScore evaluation\n",
    "\n",
    "**BERTScore** evaluates semantic similarity using contextual embeddings from a pre-trained BERT model. It compares each token in the prediction with tokens in the reference, capturing meaningful overlap rather than exact matches. Evalutating with BERTScore allows us to better accounts for paraphrasing and lexical variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629cde1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def compute_exact_match(pred, ref):\n",
    "    return int(normalize_text(pred) == normalize_text(ref))\n",
    "\n",
    "def compute_f1(pred, ref):\n",
    "    pred_tokens = word_tokenize(normalize_text(pred))\n",
    "    ref_tokens = word_tokenize(normalize_text(ref))\n",
    "\n",
    "    common = set(pred_tokens) & set(ref_tokens)\n",
    "    if not common:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens) if pred_tokens else 0\n",
    "    recall = len(common) / len(ref_tokens) if ref_tokens else 0\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def evaluate_predictions(df, ref_col=\"answer\", pred_cols=[\"zeroShot_answer\", \"zeroShot_context_answer\"]):\n",
    "    results = {}\n",
    "\n",
    "    for pred_col in pred_cols:\n",
    "        print(f\"\\nEvaluating: {pred_col}\")\n",
    "\n",
    "        preds = df[pred_col].astype(str).tolist()\n",
    "        refs = df[ref_col].astype(str).tolist()\n",
    "\n",
    "        em_scores = []\n",
    "        f1_scores = []\n",
    "        exact_matches = []\n",
    "\n",
    "        for pred, ref in zip(preds, refs):\n",
    "            em = compute_exact_match(pred, ref)\n",
    "            em_scores.append(em)\n",
    "            if em:\n",
    "                exact_matches.append((pred, ref))\n",
    "            f1_scores.append(compute_f1(pred, ref))\n",
    "\n",
    "        # BERTScore\n",
    "        P, R, F1 = bert_score(preds, refs, lang=\"en\", verbose=False)\n",
    "\n",
    "        # Print exact matches found\n",
    "        print(f\"Exact Matches ({len(exact_matches)} found):\")\n",
    "        for pred, ref in exact_matches:\n",
    "            print(f\"‚úì Prediction: {pred} | Reference: {ref}\")\n",
    "\n",
    "        results[pred_col] = {\n",
    "            \"Exact Match\": sum(em_scores) / len(em_scores),\n",
    "            \"F1 Score\": sum(f1_scores) / len(f1_scores),\n",
    "            \"BERTScore Precision\": P.mean().item(),\n",
    "            \"BERTScore Recall\": R.mean().item(),\n",
    "            \"BERTScore F1\": F1.mean().item()\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = 'subset_with_generated_answers_with_context_t5.csv'\n",
    "t5_df = pd.read_csv(file_path)\n",
    "file_path = 'subset_with_generated_answers_with_context_gemma.csv' \n",
    "gemma_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6052f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_predictions(t5_df)\n",
    "for pred_type, metrics in results.items():\n",
    "    print(f\"\\nResults for {pred_type}:\")\n",
    "    for metric, score in metrics.items():\n",
    "        print(f\"  {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6537212",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_predictions(gemma_df)\n",
    "for pred_type, metrics in results.items():\n",
    "    print(f\"\\nResults for {pred_type}:\")\n",
    "    for metric, score in metrics.items():\n",
    "        print(f\"  {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19045f26",
   "metadata": {},
   "source": [
    "### ROUGE evaluation\n",
    "\n",
    "An additional evaluation metric is **ROUGE**, or Recall-Oriented Understudy for Gisting Evaluation, it is a set of metrics and a software package used for comparing the answer generated by the Gemma and T5 models against the reference answers provided by GPT. ROUGE metrics range between 0 and 1, with higher scores indicating higher similarity between the automatically produced summary and the reference. \n",
    "\n",
    "In our evaluation, we used the following ROUGE variants to assess the overlap between the generated answers and the ground truth:\n",
    "\n",
    "- **ROUGE-1**\n",
    "Measures the overlap of unigrams (single words) between the generated and reference text.\n",
    "It reflects how well individual words are captured.\n",
    "\n",
    "- **ROUGE-2**\n",
    "Measures the overlap of bigrams (two-word sequences).\n",
    "It is more sensitive to fluency and short phrase correctness than ROUGE-1.\n",
    "\n",
    "- **ROUGE-L**\n",
    "Based on the Longest Common Subsequence (LCS).\n",
    "It captures sentence-level structure similarity and allows for some reordering of words while preserving sequence.\n",
    "\n",
    "- **ROUGE-Lsum**\n",
    "A summarization-optimized variant of ROUGE-L used in HuggingFace's implementation.\n",
    "It evaluates multi-sentence outputs as a single unit and is better suited for longer, paragraph-style answers.\n",
    "\n",
    "As we can see from the resulting plot, the aswers provided with contexts are better then the ones without as expected. Gemma is always giving quite higher scores compared to T5. \n",
    "Overally scores obtained only using two-word sequences overlap provide lower values compared to the other ROUGE metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5aa5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q evaluate absl-py  rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454fa59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def evaluate_predictions_with_rouge(df):\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    results = {}\n",
    "\n",
    "    for pred_type, col in {\n",
    "        \"zeroShot_answer\": \"zeroShot_answer\",\n",
    "        \"zeroShot_context_answer\": \"zeroShot_context_answer\"\n",
    "    }.items():\n",
    "        predictions = df[col].astype(str).tolist()\n",
    "        references = df[\"answer\"].astype(str).tolist()\n",
    "        scores = rouge.compute(predictions=predictions, references=references, use_aggregator=True)\n",
    "        results[pred_type] = scores\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca99bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_df(results, model_name):\n",
    "    data = []\n",
    "    for pred_type, metrics in results.items():\n",
    "        for metric, score in metrics.items():\n",
    "            data.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Prediction Type\": \"With Context\" if \"context\" in pred_type else \"Without Context\",\n",
    "                \"Metric\": metric,\n",
    "                \"Score\": score\n",
    "            })\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e124c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_t5 = evaluate_predictions_with_rouge(t5_df)\n",
    "results_gemma = evaluate_predictions_with_rouge(gemma_df)\n",
    "\n",
    "df_t5 = results_to_df(results_t5, \"T5\")\n",
    "df_gemma = results_to_df(results_gemma, \"Gemma\")\n",
    "\n",
    "df_all = pd.concat([df_t5, df_gemma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22250749",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "g = sns.catplot(data=df_all, x=\"Model\", y=\"Score\", hue=\"Prediction Type\",\n",
    "                col=\"Metric\", kind=\"bar\", height=4, aspect=0.9, palette=\"muted\")\n",
    "g.figure.subplots_adjust(top=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bc024a",
   "metadata": {},
   "source": [
    "## üéÅ RAG\n",
    "\n",
    "In this section we model a **RAG** (Retrieval-Augmented Generation) which is a framework that combines information retrieval with text generation. Instead of relying solely on a model‚Äôs internal knowledge, RAG retrieves relevant documents or context from an external corpus and uses this information to generate more accurate and informed responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f0ff2",
   "metadata": {},
   "source": [
    "### Retriever with all-MiniLM-L6-v2(Dense), TFIDF(Sparse) and hybrid one\n",
    "\n",
    "The retriever module is designed to identify the most 3 relevant contexts from the dataset, given a query. We employ three retrieval strategies:\n",
    "\n",
    "**Dense Retrieval (all-MiniLM-L6-v2)**: This approach encodes queries and documents into dense vector embeddings in a continuous semantic space using a pretrained transformer model. The similarity between the query and documents is computed by the cosine similarity between their embeddings. Dense retrieval captures semantic relationships beyond exact keyword overlap, enabling retrieval of relevant but lexically different contexts.\n",
    "\n",
    "**Sparse Retrieval (TF-IDF)**: Term Frequency-Inverse Document Frequency (TF-IDF) excels at precise lexical matching and is efficient for large-scale retrieval.\n",
    "\n",
    "**Hybrid Retrieval**: The hybrid retriever combines dense and sparse scores, typically using a weighted sum, to leverage the complementary strengths of both methods. This integration balances semantic understanding with exact keyword matches, enhancing overall retrieval accuracy.\n",
    "\n",
    "The combined similarity score between a query and a document is computed as a weighted sum of the dense and sparse retrieval scores:\n",
    "\n",
    "$$\n",
    "\\text{score}_{\\text{combined}}(i) = \\alpha \\times \\text{score}_{\\text{dense}}(i) + (1 - \\alpha) \\times \\text{score}_{\\text{tfidf}}(i)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $\\alpha \\in [0,1]$ is the weight balancing dense and sparse scores,  \n",
    "- $\\text{score}_{\\text{dense}}(i)$ is the dense retrieval similarity for the $i$-th context,  \n",
    "- $\\text{score}_{\\text{tfidf}}(i)$ is the TF-IDF similarity score for the $i$-th context.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada7c47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Create the dictionary and corpus for the TF-IDF model\n",
    "texts = train_df['context']\n",
    "texts = [word_tokenize(text.lower()) for text in texts]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#¬†Build the TF-IDF vectorizer\n",
    "tfidf_model = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "\n",
    "# Create the similarity index\n",
    "index = similarities.MatrixSimilarity(corpus_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f26d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sentence_embeddings for Dense Retriever with MiniLM\n",
    "# retrieves the top 3 context given the question\n",
    "\n",
    "from sentence_transformers import util\n",
    "\n",
    "def retrieve_dense(question, k=3):\n",
    "    # Compute embedding for the question\n",
    "    question_embedding = model.encode([question], convert_to_tensor=True)\n",
    "    \n",
    "    # Compute cosine similarities between question and contexts\n",
    "    cosine_scores = util.pytorch_cos_sim(question_embedding, sentence_embeddings)[0]\n",
    "    \n",
    "    # Move cosine_scores to CPU before using numpy\n",
    "    cosine_scores = cosine_scores.cpu().numpy()\n",
    "    \n",
    "    # Get top-k similar context indices\n",
    "    top_k_idx = np.argsort(cosine_scores, axis=0)[-k:][::-1]\n",
    "    \n",
    "    # Return the top-k contexts\n",
    "    return [train_df['context'].iloc[i] for i in top_k_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d3101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sentence_embeddings for Sparse Retriever with TFIDF\n",
    "# retrieves the top 3 context given the question\n",
    "\n",
    "def retrieve_tfidf(question, k=3):\n",
    "    # Tokenize and create a bag-of-words representation for the question\n",
    "    query_bow = dictionary.doc2bow(word_tokenize(question.lower()))\n",
    "    \n",
    "    # Convert it into a TF-IDF representation\n",
    "    query_tfidf = tfidf_model[query_bow]\n",
    "    \n",
    "    # Compute similarity between the query and all contexts\n",
    "    similarities_scores = index[query_tfidf]\n",
    "    \n",
    "    # Get top-k similar context indices\n",
    "    top_k = sorted(enumerate(similarities_scores), key=lambda x: x[1], reverse=True)[:k]\n",
    "    \n",
    "    # Return the top-k contexts\n",
    "    return [train_df['context'].iloc[i] for i, _ in top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62511b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_hybrid(question, k=3, alpha=0.5):\n",
    "    # Retrieve dense contexts\n",
    "    dense_contexts = retrieve_dense(question, k)\n",
    "    \n",
    "    # Retrieve tf-idf contexts\n",
    "    tfidf_contexts = retrieve_tfidf(question, k)\n",
    "    \n",
    "    # Compute dense scores for the question\n",
    "    dense_scores = util.pytorch_cos_sim(model.encode([question], convert_to_tensor=True), sentence_embeddings)[0]\n",
    "    \n",
    "    # Initialize a dictionary to store combined scores\n",
    "    combined_scores = {}\n",
    "    \n",
    "    # Combine dense and tf-idf scores\n",
    "    for i, context in enumerate(dense_contexts):\n",
    "        dense_score = dense_scores[i].item()\n",
    "        \n",
    "        # Compute the tf-idf score for the context\n",
    "        tfidf_score = index[dictionary.doc2bow(word_tokenize(context.lower()))]\n",
    "        tfidf_score = max(tfidf_score) if len(tfidf_score) > 0 else 0\n",
    "        \n",
    "        # Combine scores\n",
    "        combined_scores[i] = alpha * dense_score + (1 - alpha) * tfidf_score\n",
    "    \n",
    "    # Get top-k contexts based on the combined scores\n",
    "    top_k = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    \n",
    "    # Return the top-k contexts\n",
    "    return [train_df['context'].iloc[i] for i, _ in top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2debe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example question\n",
    "question = \"What is the impact of the porn on young people?\"\n",
    "\n",
    "# Using Dense Retriever\n",
    "top_dense_contexts = retrieve_dense(question, k=3)\n",
    "print(\"Top Dense Contexts:\")\n",
    "for context in top_dense_contexts:\n",
    "    print(context)\n",
    "\n",
    "# Using TF-IDF Retriever\n",
    "top_tfidf_contexts = retrieve_tfidf(question, k=3)\n",
    "print(\"\\nTop TF-IDF Contexts:\")\n",
    "for context in top_tfidf_contexts:\n",
    "    print(context)\n",
    "\n",
    "# Using Hybrid Retriever\n",
    "top_hybrid_contexts = retrieve_hybrid(question, k=3, alpha=0.5)\n",
    "print(\"\\nTop Hybrid Contexts:\")\n",
    "for context in top_hybrid_contexts:\n",
    "    print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571eaa28",
   "metadata": {},
   "source": [
    "### Generator with Gemma\n",
    "\n",
    "The generator answers the question using Gemma model given the best 3 matching contexes the retriever provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f448a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = subset[\"context\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_rag(query, generator_pipeline, k=3, alpha=0.5, max_new_tokens=256):\n",
    "    # Get top-k retrieved contexts\n",
    "    top_contexts = retrieve_hybrid(query, k=k, alpha=alpha)\n",
    "    context = \"\\n\\n\".join(top_contexts)\n",
    "\n",
    "    prompt = f\"\"\"Use the provided context to answer the question accurately. \n",
    "        Do not repeat the context or the question in your response. \n",
    "        Avoid generating multiple-choice options or irrelevant information. \n",
    "        Provide only the answer in a clear and concise manner.\n",
    "        \n",
    "        Context: {context}\n",
    "        Question: {query}\n",
    "        Answer:\"\"\"\n",
    "\n",
    "    # Generate answer\n",
    "    output = generator_pipeline(prompt, max_new_tokens=max_new_tokens, do_sample=True)\n",
    "    return output[0][\"generated_text\"].split(\"Answer:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d0287",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=gemma_model, tokenizer=gemma_tokenizer)\n",
    "\n",
    "# Example question\n",
    "query = \"Which are the benefits reported from having access to Self-supply water sources \"\n",
    "\n",
    "# Get RAG answer\n",
    "answer = answer_with_rag(query, generator)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebea315",
   "metadata": {},
   "source": [
    "# üïπÔ∏è Gradio & üé§ GTTS: Interactive Dashboard\n",
    "\n",
    "This section implements an interactive QA demonstration dashboard with several key features:\n",
    "\n",
    "1. **Semantic search**: Uses SentenceTransformer embeddings to find the most similar question to user queries\n",
    "2. **Comparative answer display**: Shows original, zero-shot, and context-enhanced answers side-by-side\n",
    "3. **Automated quality assessment**: Calculates and displays BERTScore F1 metrics to quantify answer quality\n",
    "4. **Text-to-speech synthesis**: Generates audio summaries of results using Google's TTS service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gradio gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2940a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import io\n",
    "from IPython.display import Audio\n",
    "\n",
    "def text_to_speech(text, language='en'):\n",
    "    mp3_fp = io.BytesIO()\n",
    "    tts = gTTS(text=text, lang=language)\n",
    "    tts.write_to_fp(mp3_fp)\n",
    "    mp3_fp.seek(0)\n",
    "    return mp3_fp.read()\n",
    "\n",
    "def play_audio(audio):\n",
    "    from IPython.display import Audio\n",
    "    return Audio(audio, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e4fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_audio(text_to_speech(\"Hello, this is a test of the text-to-speech functionality using Google text-to-speech library.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3992c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import pandas as pd\n",
    "from gtts import gTTS\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# Load model and embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "df = pd.read_csv('subset_with_generated_answers_with_context_gemma.csv')\n",
    "\n",
    "# Precompute embeddings for all questions \n",
    "question_embeddings = model.encode(df['question'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "def search_and_display(query):\n",
    "    \"\"\"Search for similar questions and return the results\"\"\"\n",
    "    if not query:\n",
    "        return \"\", \"\", \"\", \"\", \"\", \"\"\n",
    "    \n",
    "    # Encode query and find best match\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, question_embeddings)[0]\n",
    "    top_idx = torch.argmax(cosine_scores).item()\n",
    "    \n",
    "    # Get matching result\n",
    "    result = df.iloc[top_idx]\n",
    "    \n",
    "    # Generate summary for TTS\n",
    "    summary = f\"The question is: {result['question']}. The ground truth answer is: {result['answer']}. The Gemma model without using context generates: {result['zeroShot_answer']}. With context, it generates: {result['zeroShot_context_answer']}.\"\n",
    "    \n",
    "    # Return each answer separately plus the summary\n",
    "    return (\n",
    "        result['question'],\n",
    "        result['answer'],\n",
    "        result['zeroShot_answer'],\n",
    "        result['zeroShot_context_answer'],\n",
    "        result['context'],\n",
    "        summary\n",
    "    )\n",
    "\n",
    "def text_to_speech_wrapper(text):\n",
    "    \"\"\"Wrapper for text-to-speech to handle Gradio's audio output format\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "        \n",
    "    # Create an audio file in memory\n",
    "    mp3_fp = io.BytesIO()\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    tts.write_to_fp(mp3_fp)\n",
    "    mp3_fp.seek(0)\n",
    "    \n",
    "    # Return just the binary audio data\n",
    "    return mp3_fp.getvalue()\n",
    "\n",
    "def calculate_bertscores(original_answer, zeroshot_answer, context_answer):\n",
    "    \"\"\"Calculate BERTScore metrics and return them as values\"\"\"\n",
    "    if not original_answer or not zeroshot_answer or not context_answer:\n",
    "        return 0, 0\n",
    "    \n",
    "    # Calculate scores for zero-shot answer\n",
    "    zeroshot_P, zeroshot_R, zeroshot_F1 = bert_score([zeroshot_answer], [original_answer], lang=\"en\", verbose=False)\n",
    "    zeroshot_score = zeroshot_F1.item()\n",
    "    \n",
    "    # Calculate scores for context-based answer\n",
    "    context_P, context_R, context_F1 = bert_score([context_answer], [original_answer], lang=\"en\", verbose=False)\n",
    "    context_score = context_F1.item()\n",
    "    \n",
    "    return round(zeroshot_score, 3), round(context_score, 3)\n",
    "\n",
    "# Update the search and display function\n",
    "def search_and_display_with_scores(query):\n",
    "    question, answer, zeroshot, context_ans, ctx, summary = search_and_display(query)\n",
    "    \n",
    "    # Get BERTScore F1 values\n",
    "    zeroshot_score, context_score = calculate_bertscores(answer, zeroshot, context_ans)\n",
    "    \n",
    "    return question, answer, zeroshot, context_ans, ctx, summary, zeroshot_score, context_score\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(title=\"Question Answering\") as demo:\n",
    "    gr.Markdown(\"# Question Answering with Gemma\")\n",
    "    \n",
    "    # Store the summary text (hidden state)\n",
    "    summary_text = gr.State(\"\")\n",
    "    \n",
    "    # Input\n",
    "    query_input = gr.Textbox(\n",
    "        label=\"Your Question\", \n",
    "        placeholder=\"Type your question here...\",\n",
    "        lines=2\n",
    "    )\n",
    "    \n",
    "    search_button = gr.Button(\"Search\")\n",
    "    \n",
    "    # Similar question output\n",
    "    similar_question = gr.Textbox(label=\"Similar Question\", interactive=False)\n",
    "    \n",
    "    # Three separate slots for the different answers\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            original_answer = gr.Textbox(label=\"Original Answer\", interactive=False, lines=8)\n",
    "            \n",
    "        with gr.Column(scale=1):\n",
    "            zeroshot_answer = gr.Textbox(label=\"Zero-Shot Answer\", interactive=False, lines=8)\n",
    "            zeroshot_score = gr.Number(label=\"BERTScore F1\", interactive=False)\n",
    "            \n",
    "        with gr.Column(scale=1):\n",
    "            context_answer = gr.Textbox(label=\"Answer With Context\", interactive=False, lines=8)\n",
    "            context_score = gr.Number(label=\"BERTScore F1\", interactive=False)\n",
    "    \n",
    "    # Context display\n",
    "    context = gr.Textbox(label=\"Context\", interactive=False, lines=5)\n",
    "    \n",
    "    # Summary playback - keep only this button\n",
    "    with gr.Row():\n",
    "        play_summary_btn = gr.Button(\"üîä Generate audio summary\", variant=\"primary\")\n",
    "    \n",
    "    # Audio component for the summary\n",
    "    audio_summary = gr.Audio(visible=True, format=\"mp3\", label=\"Summary Audio\", autoplay=True)\n",
    "    \n",
    "    # Connect search function\n",
    "    search_outputs = [similar_question, original_answer, zeroshot_answer, context_answer, context, summary_text, zeroshot_score, context_score]\n",
    "    search_button.click(\n",
    "        fn=search_and_display_with_scores,\n",
    "        inputs=query_input,\n",
    "        outputs=search_outputs\n",
    "    )\n",
    "    \n",
    "    query_input.submit(\n",
    "        fn=search_and_display_with_scores,\n",
    "        inputs=query_input,\n",
    "        outputs=search_outputs\n",
    "    )\n",
    "    \n",
    "    # Connect only the summary button to TTS function\n",
    "    play_summary_btn.click(fn=text_to_speech_wrapper, inputs=[summary_text], outputs=[audio_summary])\n",
    "\n",
    "# Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
